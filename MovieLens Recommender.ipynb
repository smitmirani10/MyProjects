{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RecSys 2021M - Ex2 TEMPLATE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inwi1o3992TW"
      },
      "source": [
        "# Exerise 2\n",
        "\n",
        "The aims of this execise are to:\n",
        " - Make your first recommender by implementing a user-based CF (c.f. Lecture 4).\n",
        " - Make your first recommendations using Spotlight recommender toolkit on explicit data (Lecture 8)\n",
        " - Develop and evaluate baseline recommender systems (c.f. Lecture 3)\n",
        " - Start to think about explicit vs implicit learners\n",
        " - Evaluate your results using Spotlight (Lecture 6 & 7)\n",
        "\n",
        "This exercise builds on the lectures' material, namely Lectures 3, 4, 6, 7 and 8.\n",
        "\n",
        "There are 10 tasks to increase your understanding of the content of the Recommender Sytems course.  Each of these tasks have corresponding questions in the quiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys992pU79yhD"
      },
      "source": [
        "#Standard setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import List, Tuple, Sequence\n",
        "SEED=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Vlbi9W-d2j"
      },
      "source": [
        "We'll be using Movielens again. Let's load it in to the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg093kVRAvHw",
        "outputId": "e43e760f-8ef2-4d4f-84c8-4e223f42d8d8"
      },
      "source": [
        "!curl -o ml-latest-small.zip http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "# backup location\n",
        "#!curl -o ml-latest-small.zip http://www.dcs.gla.ac.uk/~craigm/recsysHM/ml-latest-small.zip\n",
        "!unzip -o ml-latest-small.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  955k  100  955k    0     0   891k      0  0:00:01  0:00:01 --:--:--  891k\n",
            "Archive:  ml-latest-small.zip\n",
            "   creating: ml-latest-small/\n",
            "  inflating: ml-latest-small/links.csv  \n",
            "  inflating: ml-latest-small/tags.csv  \n",
            "  inflating: ml-latest-small/ratings.csv  \n",
            "  inflating: ml-latest-small/README.txt  \n",
            "  inflating: ml-latest-small/movies.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yss68U_EA0w2"
      },
      "source": [
        "ratings_df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "movies_df = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
        "\n",
        "# we're going to treat userId as strings, and similarly as movies. This will prevent confusion later on.\n",
        "ratings_df['userId'] =  \"u\" + ratings_df['userId'].astype(str)\n",
        "ratings_df['movieId'] = \"m\" + ratings_df['movieId'].astype(str)\n",
        "movies_df['movieId'] = \"m\" +  movies_df['movieId'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5QQBZ3QS2Hv",
        "outputId": "4bae84e6-ad5a-48fe-c113-0307904eca6b"
      },
      "source": [
        "ratings_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u1</td>\n",
              "      <td>m1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u1</td>\n",
              "      <td>m3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u1</td>\n",
              "      <td>m6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u1</td>\n",
              "      <td>m47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u1</td>\n",
              "      <td>m50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  userId movieId  rating  timestamp\n",
              "0     u1      m1     4.0  964982703\n",
              "1     u1      m3     4.0  964981247\n",
              "2     u1      m6     4.0  964982224\n",
              "3     u1     m47     5.0  964983815\n",
              "4     u1     m50     5.0  964982931"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "eMfsveWjS4xP",
        "outputId": "e5181cb2-7426-49fe-c336-e57ad23d6987"
      },
      "source": [
        "movies_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>m1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>m2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>m3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>m4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>m5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  movieId  ...                                       genres\n",
              "0      m1  ...  Adventure|Animation|Children|Comedy|Fantasy\n",
              "1      m2  ...                   Adventure|Children|Fantasy\n",
              "2      m3  ...                               Comedy|Romance\n",
              "3      m4  ...                         Comedy|Drama|Romance\n",
              "4      m5  ...                                       Comedy\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rAIXwDoBCda"
      },
      "source": [
        "# Part A. User-based CF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isjp_rS0BCgh"
      },
      "source": [
        "You can generate a matrix of ratings with the ratings_df dataframe. In the matrix, the unrated items are filled with 0 (this means they have no impact upon the calculated Cosine value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKsmW549BNlq",
        "outputId": "cd2f59ed-99ff-4bde-9f4e-e49393d6f544"
      },
      "source": [
        "r_df_matrix = ratings_df.pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n",
        "r_df_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>movieId</th>\n",
              "      <th>m1</th>\n",
              "      <th>m10</th>\n",
              "      <th>m100</th>\n",
              "      <th>m100044</th>\n",
              "      <th>m100068</th>\n",
              "      <th>m100083</th>\n",
              "      <th>m100106</th>\n",
              "      <th>m100159</th>\n",
              "      <th>m100163</th>\n",
              "      <th>m100194</th>\n",
              "      <th>m100226</th>\n",
              "      <th>m100277</th>\n",
              "      <th>m1003</th>\n",
              "      <th>m100302</th>\n",
              "      <th>m100304</th>\n",
              "      <th>m100306</th>\n",
              "      <th>m100326</th>\n",
              "      <th>m100383</th>\n",
              "      <th>m100390</th>\n",
              "      <th>m100397</th>\n",
              "      <th>m1004</th>\n",
              "      <th>m100487</th>\n",
              "      <th>m100498</th>\n",
              "      <th>m1005</th>\n",
              "      <th>m100507</th>\n",
              "      <th>m100527</th>\n",
              "      <th>m100553</th>\n",
              "      <th>m100556</th>\n",
              "      <th>m100579</th>\n",
              "      <th>m1006</th>\n",
              "      <th>m100611</th>\n",
              "      <th>m1007</th>\n",
              "      <th>m100714</th>\n",
              "      <th>m100737</th>\n",
              "      <th>m1008</th>\n",
              "      <th>m100810</th>\n",
              "      <th>m100843</th>\n",
              "      <th>m100882</th>\n",
              "      <th>m1009</th>\n",
              "      <th>m100906</th>\n",
              "      <th>...</th>\n",
              "      <th>m98836</th>\n",
              "      <th>m98908</th>\n",
              "      <th>m98961</th>\n",
              "      <th>m99</th>\n",
              "      <th>m990</th>\n",
              "      <th>m99005</th>\n",
              "      <th>m99007</th>\n",
              "      <th>m99030</th>\n",
              "      <th>m99087</th>\n",
              "      <th>m991</th>\n",
              "      <th>m99106</th>\n",
              "      <th>m99112</th>\n",
              "      <th>m99114</th>\n",
              "      <th>m99117</th>\n",
              "      <th>m99122</th>\n",
              "      <th>m99130</th>\n",
              "      <th>m99145</th>\n",
              "      <th>m99149</th>\n",
              "      <th>m99191</th>\n",
              "      <th>m993</th>\n",
              "      <th>m994</th>\n",
              "      <th>m99415</th>\n",
              "      <th>m99437</th>\n",
              "      <th>m99532</th>\n",
              "      <th>m99574</th>\n",
              "      <th>m996</th>\n",
              "      <th>m99636</th>\n",
              "      <th>m99638</th>\n",
              "      <th>m99721</th>\n",
              "      <th>m99728</th>\n",
              "      <th>m99750</th>\n",
              "      <th>m99764</th>\n",
              "      <th>m998</th>\n",
              "      <th>m99813</th>\n",
              "      <th>m99846</th>\n",
              "      <th>m99853</th>\n",
              "      <th>m999</th>\n",
              "      <th>m99910</th>\n",
              "      <th>m99917</th>\n",
              "      <th>m99992</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>userId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>u1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u100</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u101</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u102</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u95</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u96</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u97</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u98</th>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u99</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>610 rows × 9724 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "movieId   m1  m10  m100  m100044  m100068  ...  m99853  m999  m99910  m99917  m99992\n",
              "userId                                     ...                                      \n",
              "u1       4.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u10      0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u100     0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u101     0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u102     0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "...      ...  ...   ...      ...      ...  ...     ...   ...     ...     ...     ...\n",
              "u95      0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u96      5.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u97      0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u98      4.5  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u99      0.0  4.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "\n",
              "[610 rows x 9724 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HRc8W_935Hc",
        "outputId": "f0024973-11ea-4071-b056-6354b8746941"
      },
      "source": [
        "total_users=len(r_df_matrix.index)\n",
        "total_users"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpJXEqQ_BKxg"
      },
      "source": [
        "The left hand bold column is the [index of the dataframe](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) - that is, an attribute of the dataframe that allows fast lookup of rows. In this case, userId has become our index column.\n",
        "\n",
        "You can get all the index of users using the .index "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRIXYwQgBRt2",
        "outputId": "6bde95ba-5682-4b80-bfc2-6323ed736221"
      },
      "source": [
        "r_df_matrix.index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['u1', 'u10', 'u100', 'u101', 'u102', 'u103', 'u104', 'u105', 'u106',\n",
              "       'u107',\n",
              "       ...\n",
              "       'u90', 'u91', 'u92', 'u93', 'u94', 'u95', 'u96', 'u97', 'u98', 'u99'],\n",
              "      dtype='object', name='userId', length=610)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAZdk6aK4m8p",
        "outputId": "09407467-4e23-4598-8a6c-04ffc4e62cb1"
      },
      "source": [
        "user_list=r_df_matrix.index.tolist()\n",
        "len(user_list)\n",
        "#user_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "610"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8RcZo1bBU8O"
      },
      "source": [
        "You can also use [.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) to access rows, by their \"index\". For instance, we can get all ratings of a specific user with userId=‘1’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77XOSOdqBXmw",
        "outputId": "a30cc3b3-2105-457f-d329-b5da9eb9fc79"
      },
      "source": [
        "indices=r_df_matrix.loc['u1'].values.nonzero()[0]\n",
        "#indices[0][2]\n",
        "indices[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9_SlC4Beut"
      },
      "source": [
        "User-based CF heavily relies upon Cosine similarity. We are providing a Cosine similarity implementation based on numpy operations. We also show how to use df.loc to get all the ratings of a given user from `r_df_matrix` as a Series - we then make this into a numpy array using the [.values](https://pandas.pydata.org/docs/reference/api/pandas.Series.values.html) property.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NXXbCQsBbJW",
        "outputId": "75968005-d1cc-45eb-de87-b7e605570e5a"
      },
      "source": [
        "def cos_sim(a, b):\n",
        "  from numpy.linalg import norm\n",
        "  from numpy import dot\n",
        "  return dot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "print('Cosine similarity between userId=1 and itself is:')\n",
        "print(cos_sim(r_df_matrix.loc['u1'].values, r_df_matrix.loc['u1'].values))\n",
        "\n",
        "print('Cosine similarity between userId=1 and userId=607 is:')\n",
        "print(cos_sim(r_df_matrix.loc['u1'].values, r_df_matrix.loc['u607'].values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity between userId=1 and itself is:\n",
            "1.0\n",
            "Cosine similarity between userId=1 and userId=607 is:\n",
            "0.2693892401115333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B061AiTiBlWJ"
      },
      "source": [
        "## Task 1. Get the most similar users.\n",
        "\n",
        "User-based CF is based on user-neighbourhoods. In this task, you will implement a function ` get_most_similar_users(userId : str, k : int = 10)` that will identify the userIds of the k most similar users to the specified userId, and their corresponding cosine similarities. \n",
        "\n",
        "In determining the most similar users, you should break ties based on their position in the array - for instance, if two users are tied as 2nd most similar user, the user who appears earlier should be 2nd, and the latter user third.\n",
        "\n",
        "You should exclude the compared user itself when generating a list of the most similar users.\n",
        "\n",
        "NB: We are using Python type hints to remind you what the function parameters (`str`, `int`) and return type (`Tuple[Sequence[str], Sequence[float]]`) should be.\n",
        "\n",
        "Hints: \n",
        " - The cos_sim function should be used here. \n",
        " - Higher cos_sim means more similar.\n",
        " - Try SciPy's [`rankdata()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html) function. Given an array, `rankdata()` tells you positions in sorted rank order. For instance:\n",
        "```\n",
        ">>> rankdata([5.9, 2.1, 4.3])\n",
        "array([3., 1., 2.])\n",
        "```\n",
        "It also has support for addressing ties.\n",
        " - The first return component of [np.nonzero](https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html) can be used to return the indices of the elements that are non-zero. E.g.\n",
        " ```\n",
        " >>> np.array([True,False]).nonzero()[0]\n",
        "array([0])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj1wpnS7BbMF",
        "outputId": "3e1e4420-40de-44de-bc17-29866669d1e9"
      },
      "source": [
        "from scipy.stats import rankdata\n",
        "\n",
        "def get_most_similar_users(userId : str, k : int = 10) -> Tuple[Sequence[str], Sequence[float]]:\n",
        "  similarity=[]\n",
        "  userid=[]\n",
        "  topk_userids=[]\n",
        "  topk_cosines=[]\n",
        "  for j in user_list:\n",
        "    if (j != userId):\n",
        "      temp_sim=cos_sim(r_df_matrix.loc[userId].values, r_df_matrix.loc[j].values)\n",
        "      similarity.append(temp_sim)\n",
        "      userid.append(j)\n",
        "  rank=rankdata(similarity, method='ordinal').astype(int)\n",
        "  rank=len(rank) - rank\n",
        "  rank=rank+1\n",
        "  rank=rank.tolist()\n",
        "  #print(rank)\n",
        "  #print(similarity)\n",
        "  #print(userid)\n",
        "  #print(\"Max index:\",similarity.index(max(similarity)))\n",
        "  for s in range(1,k+1):\n",
        "    r=rank.index(s)\n",
        "    #print(\"Index is \",r)\n",
        "    topk_userids.append(userid[r])\n",
        "    topk_cosines.append(similarity[r])\n",
        "  return (topk_userids, topk_cosines)\n",
        "\n",
        "print(\"Most similar user to u1 is\",get_most_similar_users(userId='u10', k=1))\n",
        "\n",
        "# Add your solution here (cosine similarity > 0)\n",
        "#0.07168754454516074\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most similar user to u1 is (['u159'], [0.28826463078187997])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3jnW2BCAIWR"
      },
      "source": [
        "You can now answer the questions corresponding to Task 1 in the quiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVnsPTKjzqnD",
        "outputId": "748074f2-57f3-4f87-9410-4d3c20351467"
      },
      "source": [
        "print(get_most_similar_users(userId='u10', k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['u159'], [0.28826463078187997])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIMwvzqJ2YuE",
        "outputId": "e375f476-2d62-4cfd-dafb-95d85ddc050c"
      },
      "source": [
        "print(get_most_similar_users(userId='u500', k=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['u453', 'u45'], [0.27983214052930266, 0.26236874974336444])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZepSCnBwc7"
      },
      "source": [
        "## Task 2. Predict ratings via user-based CF.\n",
        "\n",
        "Now you should implement your user-based CF, within a predict() function. \n",
        "The aim of this function is to predict the rating of a given userId for a given itemId.\n",
        "\n",
        "Your implementation should make use of your `get_most_similar_users()` implementation above, using k=10 nearest neighbours.\n",
        "\n",
        "Hint: \n",
        " - You may wish to revise user-based CF from Lecture 4. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eX33RkTBbS2",
        "outputId": "88beb917-8479-40e4-9b78-db947ab7df23"
      },
      "source": [
        "def predict_rating(userId : str, movieId : str) -> float:\n",
        "  k=10\n",
        "  v=[]\n",
        "  numerator=0\n",
        "  denominator=0\n",
        "  topk_userids,topk_cosines=get_most_similar_users(userId, k)\n",
        "  print(topk_userids)\n",
        "  print(topk_userids.index('u45'))\n",
        "  for i in topk_userids:\n",
        "    v_rating=r_df_matrix.loc[i][movieId]\n",
        "    v.append(v_rating)\n",
        "  #print(v)\n",
        "  for j in range(0,10):\n",
        "    numerator = numerator+(round(topk_cosines[j],2)*v[j])\n",
        "    denominator=denominator+topk_cosines[j]\n",
        "  predicted = (numerator)/(denominator)# predicted rating value\n",
        "  return predicted\n",
        "\n",
        "print(\"Predicted rating:\", predict_rating(userId='u1', movieId='m1'))\n",
        "\n",
        "print(\"Actual rating:\", r_df_matrix.loc['u1']['m1'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['u266', 'u313', 'u368', 'u57', 'u91', 'u469', 'u39', 'u288', 'u452', 'u45']\n",
            "9\n",
            "Predicted rating: 2.3417342729587767\n",
            "Actual rating: 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuWLuAZzB9br"
      },
      "source": [
        "You can complete answering the quiz questions for Task 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqfUghVsCA7G"
      },
      "source": [
        "## Task 3. Predict ratings via user-based CF with Mean-center normalisation.\n",
        "\n",
        "Users usually rate differently: (1) some rate high, while others low. (2) Some use more of the scale than others. However, the user-based CF we implemented above ignores these differences. To this end, we can apply normalisation to compensate. In this task, you will implement user-based CF with Mean-Center Normalisation.\n",
        "\n",
        "Provide implementations for `mean_rating(userId : str)` and `predict_rating_MC(userId : str, movieId : str)`.\n",
        "\n",
        "Hints: \n",
        "- See lecture 4 about user-based CF with Mean-center normalisation. \n",
        "- Check if the predicted rating for a given user makes sense (i.e. what did the user rate before)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq3cNPVX-nqs",
        "outputId": "9e86b2b8-605b-4490-f265-588177209f86"
      },
      "source": [
        "ratings_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u1</td>\n",
              "      <td>m1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u1</td>\n",
              "      <td>m3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u1</td>\n",
              "      <td>m6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u1</td>\n",
              "      <td>m47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u1</td>\n",
              "      <td>m50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100831</th>\n",
              "      <td>u610</td>\n",
              "      <td>m166534</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1493848402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100832</th>\n",
              "      <td>u610</td>\n",
              "      <td>m168248</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1493850091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100833</th>\n",
              "      <td>u610</td>\n",
              "      <td>m168250</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1494273047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100834</th>\n",
              "      <td>u610</td>\n",
              "      <td>m168252</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1493846352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100835</th>\n",
              "      <td>u610</td>\n",
              "      <td>m170875</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1493846415</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100836 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       userId  movieId  rating   timestamp\n",
              "0          u1       m1     4.0   964982703\n",
              "1          u1       m3     4.0   964981247\n",
              "2          u1       m6     4.0   964982224\n",
              "3          u1      m47     5.0   964983815\n",
              "4          u1      m50     5.0   964982931\n",
              "...       ...      ...     ...         ...\n",
              "100831   u610  m166534     4.0  1493848402\n",
              "100832   u610  m168248     5.0  1493850091\n",
              "100833   u610  m168250     5.0  1494273047\n",
              "100834   u610  m168252     5.0  1493846352\n",
              "100835   u610  m170875     3.0  1493846415\n",
              "\n",
              "[100836 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q5Efb4zB5_O",
        "outputId": "7eae0604-8df4-4357-f754-167c21b52913"
      },
      "source": [
        "def mean_rating(userId : str) -> float:\n",
        "  sum=0\n",
        "  total=0\n",
        "  j=0\n",
        "  indices=r_df_matrix.loc[userId].values.nonzero()[0]\n",
        "  for i in indices:\n",
        "    rating=r_df_matrix.loc[userId].values[i]\n",
        "    sum=sum+rating\n",
        "    j=j+1\n",
        "  mean_rating = round((sum/j),2) # mean-centering value\n",
        "  return mean_rating\n",
        "\n",
        "print(\"Mean rating of user u5:\", mean_rating('u5') )\n",
        "\n",
        "def predict_rating_MC(userId : str, movieId : str) -> float:\n",
        "  user_mean=mean_rating(userId)\n",
        "  topk_userids,topk_cosines=get_most_similar_users(userId, k=10)\n",
        "  numerator=0\n",
        "  denominator=0\n",
        "  for i in topk_userids:\n",
        "    mean=mean_rating(i)\n",
        "    index=topk_userids.index(i)\n",
        "    sim=topk_cosines[index]\n",
        "    v_rating=r_df_matrix.loc[i][movieId]\n",
        "    numerator=numerator+(sim*(v_rating-mean))\n",
        "    denominator=denominator+sim\n",
        "  predicted=round(user_mean+(numerator/denominator),2)\n",
        "  #predicted = \"?\" # predicted rating value with mean-centering\n",
        "  return predicted\n",
        "\n",
        "print(\"Predicted rating:\", predict_rating_MC('u1', 'm1'))\n",
        "print(\"Actual rating:\", r_df_matrix.loc['u1']['m1'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean rating of user u5: 3.64\n",
            "Predicted rating: 3.13\n",
            "Actual rating: 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VApK70ld1tQ_"
      },
      "source": [
        "Now answer the questions for Task 3 in the quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_OpdQYHmTb"
      },
      "source": [
        "#Part B - Explicit Matrix Factorisation using Spotlight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17h6DqmGGh2f"
      },
      "source": [
        "In this part, we will investigate explicit matrix factorisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0oobMGk5eqv"
      },
      "source": [
        "We're going to use the Spotlight library - see https://github.com/maciejkula/spotlight - and its documentation at https://maciejkula.github.io/spotlight/\n",
        "\n",
        "You can install this direct from Git, but using Craig's patched version as done below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDObURPI-Shz",
        "outputId": "edf934e6-e87a-4b07-ed5f-88b0384eecb4"
      },
      "source": [
        "!pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spotlight\n",
            "  Cloning https://github.com/cmacdonald/spotlight.git (to revision master) to /tmp/pip-install-zxipnqro/spotlight_75defef1a1a44315865b3dc62f0d6d22\n",
            "  Running command git clone -q https://github.com/cmacdonald/spotlight.git /tmp/pip-install-zxipnqro/spotlight_75defef1a1a44315865b3dc62f0d6d22\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spotlight) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->spotlight) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqoiteq7IJzJ"
      },
      "source": [
        "Now we can get onto some real recommendation work. Spotlight has a handy [Interactions](https://maciejkula.github.io/spotlight/interactions.html) object, which encapsulates the basics of a recommendation dataset.\n",
        "\n",
        "In fact, there are handy loaders for a few standard datasets including MovieLens, but let's make our own, so that we can match back to the dataframe.\n",
        "\n",
        "Interactions need numbers as userids and itemids. Unfortunately, our MovieLens uses numbers, but these aren't consecutive (i.e. we have missing movieIds values). They are also strings (i.e. movieIds start with \"m\" and userIds start with \"u\").\n",
        "\n",
        "Hence, for both movies and users, we will use [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) to convert the MovieLens strings down to consecutive integers for use in Spotlight, in the `uid_map` and `iid_map` objects. We'll keep the reverse mapping around too, in case we want to lookup the actual movieId given the uid recorded by Spotlight (etc).\n",
        "\n",
        "*NB*: This is a really important concept to understand. Put simply, WE -- as humans -- deal with external representations (userId, movieId, in this dataset prefixed with \"u\" and \"m\" respectively). On the other hand, Spotlight can only deal with integers starting from 0 for both items and users (we call these \"iids\" and \"uids\"). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "ihLbjpcD-vtz",
        "outputId": "d662cc12-debd-4dc6-8783-3d3f67f0c99c"
      },
      "source": [
        "ratings_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u1</td>\n",
              "      <td>m1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u1</td>\n",
              "      <td>m3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u1</td>\n",
              "      <td>m6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u1</td>\n",
              "      <td>m47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u1</td>\n",
              "      <td>m50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100831</th>\n",
              "      <td>u610</td>\n",
              "      <td>m166534</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1493848402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100832</th>\n",
              "      <td>u610</td>\n",
              "      <td>m168248</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1493850091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100833</th>\n",
              "      <td>u610</td>\n",
              "      <td>m168250</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1494273047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100834</th>\n",
              "      <td>u610</td>\n",
              "      <td>m168252</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1493846352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100835</th>\n",
              "      <td>u610</td>\n",
              "      <td>m170875</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1493846415</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100836 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       userId  movieId  rating   timestamp\n",
              "0          u1       m1     4.0   964982703\n",
              "1          u1       m3     4.0   964981247\n",
              "2          u1       m6     4.0   964982224\n",
              "3          u1      m47     5.0   964983815\n",
              "4          u1      m50     5.0   964982931\n",
              "...       ...      ...     ...         ...\n",
              "100831   u610  m166534     4.0  1493848402\n",
              "100832   u610  m168248     5.0  1493850091\n",
              "100833   u610  m168250     5.0  1494273047\n",
              "100834   u610  m168252     5.0  1493846352\n",
              "100835   u610  m170875     3.0  1493846415\n",
              "\n",
              "[100836 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF89PzxNHrHq",
        "outputId": "1ad954e0-257c-4162-f8dd-27dff0c905a9"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "#create userId -> uid mapping dictionary. the next assigned value is the current size.\n",
        "uid_map = defaultdict(count().__next__)\n",
        "#ditto for movieId -> iid\n",
        "iid_map = defaultdict(count().__next__)\n",
        "\n",
        "#uids is an array of integers corresponding to the userId for every row in ratings_df\n",
        "#uid_map does the assignment of new uid values, or reusing the uid value assigned for\n",
        "#each userId\n",
        "uids = np.array([uid_map[uid] for uid in ratings_df[\"userId\"].values ], dtype=np.int32)\n",
        "#similar for iids\n",
        "iids = np.array([iid_map[iid] for iid in ratings_df[\"movieId\"].values ], dtype=np.int32)\n",
        "\n",
        "#freeze uid_map and iid_map so no more mappings are created\n",
        "uid_map.default_factory = None\n",
        "iid_map.default_factory = None\n",
        "\n",
        "#reverse them, so we can go from iid (int) to itemId (str)\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "num_items = len(iid_map)\n",
        "num_users = len(uid_map)\n",
        "\n",
        "print(\"%d users %d item\" % (num_users, num_items))\n",
        "\n",
        "ratings = ratings_df[\"rating\"].values.astype(np.float32)\n",
        "timestamps = ratings_df[\"timestamp\"].values.astype(np.int32)\n",
        "\n",
        "print(\"userId %s got uid %d\" % (\"u556\", uid_map[\"u556\"]))\n",
        "print(\"movieId %s got iid %d\" % (\"m54001\", iid_map[\"m54001\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "610 users 9724 item\n",
            "userId u556 got uid 555\n",
            "movieId m54001 got iid 2518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrhni5wSX7QP"
      },
      "source": [
        "Furthemore, we will use user u556 as one of our illustrative users. You will remember from Exercise 1 that they rated a number of fantasy movies highly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDwZYy2xY3-D"
      },
      "source": [
        "## On towards MF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMejAIwPNVrU"
      },
      "source": [
        "Now let's build a Spotlight [Interactions](https://maciejkula.github.io/spotlight/interactions.html) object. This contains everything that Spotlight needs to train a model. We can split it up randomly into train and test subsets \n",
        "\n",
        "NB: we use a SEED (20) to make our results reproducible. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg2tNWwPIBfu",
        "outputId": "24bca5b5-ff3c-40d1-bf4c-6ebe8690c98a"
      },
      "source": [
        "from spotlight.interactions import Interactions\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "dataset = Interactions(user_ids=uids,\n",
        "                                  item_ids=iids,\n",
        "                                  ratings=ratings,\n",
        "                                  timestamps=timestamps)\n",
        "\n",
        "#lets initialise the seed, so that its repeatable and reproducible \n",
        "train, test = random_train_test_split(dataset, random_state=np.random.RandomState(SEED))\n",
        "iids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    1,    2, ..., 3121, 1392, 2873], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1shoeRmWKXxS"
      },
      "source": [
        "Let's see how big the two datasets are. What is the train/test split percentage size?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcjOWJ-qIEge",
        "outputId": "471efa42-86f4-429d-b8df-6c0b14ef80fa"
      },
      "source": [
        "print(train)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Interactions dataset (610 users x 9724 items x 80668 interactions)>\n",
            "<Interactions dataset (610 users x 9724 items x 20168 interactions)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBnevuHnT57k"
      },
      "source": [
        "Here, you can see that following the collaborative filtering task model (see Lecture 6), all users, and all items, are present in both training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTyhLswNulTX"
      },
      "source": [
        "Now, you can think of the Interaction objects are being the partitions of the rating matrix. But we don't store it as a single big matrix. Instead, we record three one-dimensional arrays:\n",
        " \n",
        "  * one for the ids of the users\n",
        "  * one for the ids of the items\n",
        "  * one for the actual rating values.\n",
        "\n",
        "Each of these arrays is the size of the number of ratings (80668 for the training set).\n",
        "\n",
        "In essence, Interactions is a sparse matrix - for each rating, we record its x and y position, as well as the rating itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74iQtBOoUZJ1",
        "outputId": "7d66cee0-013d-4900-d5b5-b00c8307bdda"
      },
      "source": [
        "print(train.item_ids.shape)\n",
        "print(train.user_ids.shape)\n",
        "print(train.ratings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80668,)\n",
            "(80668,)\n",
            "(80668,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V9Hx3dhUjI1"
      },
      "source": [
        "For instance, let's look at the first rating:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz46dNMkUrCC",
        "outputId": "18c52ba3-a952-49c4-b14e-c4c1c831d86f"
      },
      "source": [
        "print(\"uid %d gave iid %d a rating of %d\" % (train.user_ids[0], train.item_ids[0],train.ratings[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uid 56 gave iid 1491 a rating of 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4R1mQgSUZ86"
      },
      "source": [
        "Let's take our favourite fantasy adventure fan from Exercise 1, userId u556. We can give a look at their training ratings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47QmgWtYvM4u",
        "outputId": "9e886b64-a496-4056-fa13-d292697eb02d"
      },
      "source": [
        "# map userId to the internal uid value\n",
        "userId = \"u556\"\n",
        "uid = uid_map.get(userId)\n",
        "\n",
        "# see which ratings are for this user. Use this to filter the item and ratings arrays. \n",
        "# here we are filtering a numpy array based on an array of True/False values. Its just\n",
        "# like filtering a Pandas data frame.\n",
        "print(train.item_ids[train.user_ids == uid])\n",
        "print(train.ratings[train.user_ids == uid])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6082 6087  457 1925 7951 1132  764 5989  753 1342 1893 3076 3258 1182\n",
            " 1938 1894 4796  926  770 8659 2059  917 1077  912  779  322 1307 3087\n",
            " 2518  774]\n",
            "[4.  3.5 5.  5.  4.  4.  4.  4.  4.5 4.  4.  4.5 4.  4.  4.5 3.5 4.  4.\n",
            " 4.  4.  4.  3.5 5.  2.5 4.  5.  4.  4.  4.  4. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhnKAa-KKclT"
      },
      "source": [
        "We can now learn a model. Let's start with a matrix factorisation for explicit data.  We train the model using the `fit` method. This is just like the `fit` in Sklearn - we're fitting  a model to the specified training data.\n",
        "\n",
        "This might take upto a minute. \n",
        "\n",
        "**NB:**  Spotlight can support using GPUs which we could use to slightly speed up training time, but that will make our life more difficult later on, so let's ignore this for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UduCmnlbKt-O",
        "outputId": "2d2349bb-35ac-4ac9-ed39-ff37b4ebbea5"
      },
      "source": [
        "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
        "import time  \n",
        "\n",
        "emodel = ExplicitFactorizationModel(n_iter=10,\n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
        ")\n",
        "current = time.time()\n",
        "\n",
        "emodel.fit(train, verbose=True)\n",
        "\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds \"% (diff))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 4.3081090674747395\n",
            "Epoch 1: loss 0.8099101063194154\n",
            "Epoch 2: loss 0.5096786571077153\n",
            "Epoch 3: loss 0.3636633798102789\n",
            "Epoch 4: loss 0.2919712789073775\n",
            "Epoch 5: loss 0.25697739233699024\n",
            "Epoch 6: loss 0.23643478482395788\n",
            "Epoch 7: loss 0.22271784451566165\n",
            "Epoch 8: loss 0.2139979781983774\n",
            "Epoch 9: loss 0.20728877597028697\n",
            "Training took 17 seconds \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTK1BlbLL_t"
      },
      "source": [
        "How well did we do. Well, let's give a look at the recommentations, for our specific user, userId u556. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjyVPjoGLoy6",
        "outputId": "56690e6d-8c38-4214-8b8e-02075e3c9e52"
      },
      "source": [
        "userId = \"u556\"\n",
        "\n",
        "# convert the string to the internal integer\n",
        "uid = uid_map.get(userId)\n",
        "print(\"One test item_id for userId %s (uid %d) is \" % (userId, uid))\n",
        "\n",
        "# pick one rating that the user made\n",
        "testItemId = test.item_ids[test.user_ids == uid][0] \n",
        "print(\"Test movieId is %s iid %d \" % (iid_rev_map.get(testItemId), testItemId ) )\n",
        "\n",
        "\n",
        "#here 0 is a dummy item, which Spotlight needs for some reason...\n",
        "#we discard its prediction using [1]\n",
        "predicted = emodel.predict( np.array([uid]), item_ids=np.array([0, testItemId]) )[1]\n",
        "\n",
        "#what was the actual score of the user for that movie?\n",
        "#we can get the appropriate row from the ratings dataframe, then extract that value\n",
        "actual = ratings_df[(ratings_df.movieId==iid_rev_map.get(testItemId)) & (ratings_df.userId==userId)][\"rating\"].values[0]\n",
        "\n",
        "\n",
        "def getMovieTitle(iid):\n",
        "  return movies_df[movies_df['movieId'] == iid_rev_map.get(iid)][\"title\"].values[0]\n",
        "\n",
        "print(\"Predicted rating for '%s' was %f, actual rating %0.1f, error was %f\" % (getMovieTitle(testItemId), predicted, actual, abs(predicted-actual) )) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One test item_id for userId u556 (uid 555) is \n",
            "Test movieId is m74530 iid 8141 \n",
            "Predicted rating for 'Percy Jackson & the Olympians: The Lightning Thief (2010)' was 2.574092, actual rating 3.5, error was 0.925908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWdWW8QhacOw"
      },
      "source": [
        "So this is interesting - while we saw above that the users liked fantasy movies, we predicted a rating of $\\sim 2.5$, but the user gave this particular movie a 3.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgX01Wwlr_WA"
      },
      "source": [
        "We can also ask for **all** of the recommendations for a given user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz28wrmIsDa-",
        "outputId": "13448b67-54cd-492f-db98-20328e923a36"
      },
      "source": [
        "allpreds = emodel.predict( np.array([uid]) )\n",
        "\n",
        "print(allpreds)\n",
        "print(allpreds.size)\n",
        "\n",
        "#we can recover the original rating for our test item \n",
        "print(allpreds[testItemId])\n",
        "\n",
        "# lets just check we got the correct prediction\n",
        "print(allpreds[testItemId] - actual < 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.9689248  4.3499784  4.5101566  ... 0.87423515 2.7873065  0.9850692 ]\n",
            "9724\n",
            "2.5740924\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d25P7AtBPgXS"
      },
      "source": [
        "## Latent Factors aka Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyL5EG65TuNo"
      },
      "source": [
        "Let's see how these recommendations are made. Remember from Lecture 8 that the prediction is made based on the dot product of the user's and item's latent factors (also know as \"embeddings\").\n",
        "\n",
        "We can access these embeddings directly from the emodel object. Each embedding has 32 dimensions, which is what we set when configuring Spotlight's Explicit Factorisation Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf2Em9KSa74G",
        "outputId": "fe9bb7e4-dc70-4a86-edde-17e4cdfb2d43"
      },
      "source": [
        "#the embedding of an item is a PyTorch tensor of size 32\n",
        "#a PyTorch tensor can be thought of having similar semantics as an numpy array.\n",
        "print(emodel._net.item_embeddings.weight[0].shape)\n",
        "emodel._net.item_embeddings.weight[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1223, -0.3951, -0.3488,  0.0474,  0.7867, -0.0242,  0.2448,  0.7672,\n",
              "        -0.1924, -0.0686, -0.1228,  0.6061, -0.1798, -0.3621,  0.7326,  0.2025,\n",
              "        -0.1660, -0.3077, -0.3590, -0.3852,  0.2369, -0.6257,  0.7370,  0.8468,\n",
              "         0.0755, -0.4360, -0.1154, -0.2451, -0.0357, -0.0060,  0.1001,  0.2164],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKlJIBoVNYr"
      },
      "source": [
        "We can check how Spotlight makes its prediction. The key line is https://github.com/maciejkula/spotlight/blob/master/spotlight/factorization/representations.py#L89\n",
        "\n",
        "This takes the (dot-)product of the user's \"embedding\" (latent factor) and the item's embedding. On top of these are added \"user_biases\" and \"item_biases\". What do you think these last two components are for?\n",
        "\n",
        "Let's reproduce this for our favourite user..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g14v62m4YRyd",
        "outputId": "e204c640-775c-4aac-a9ad-5565aa8f9309"
      },
      "source": [
        "# uid=555 for u556\n",
        "# testItemId is our item of interest\n",
        "\n",
        "dotprod = (emodel._net.user_embeddings.weight[uid] * emodel._net.item_embeddings.weight[testItemId]).sum(0)\n",
        "user_bias = emodel._net.user_biases(torch.tensor([uid]))\n",
        "item_bias = emodel._net.item_biases(torch.tensor([testItemId], dtype=torch.long))\n",
        "\n",
        "print(getMovieTitle(testItemId))\n",
        "\n",
        "dotprod + user_bias + item_bias"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percy Jackson & the Olympians: The Lightning Thief (2010)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.5741]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VE2tpcJb8e7"
      },
      "source": [
        "## Task 4. Examining Latent Factors\n",
        "\n",
        "Let's give a look at item-item similarities. Write a function `mostsimilar(targetMovieId, model)` that identifies the most similar movieId to the specified target, based on the Cosine similarity of their item embedding vectors. \n",
        "\n",
        "What's the closest movie to \"Harry Potter and the Deathly Hallows: Part 1 (2010)\" , which is movieId m81834 in the MovieLens dataset?\n",
        "\n",
        "Hint: \n",
        " - Since we're working with PyTorch tenors, you should use [`nn.functional.cosine_similarity(x, y, dim=0)`](https://pytorch.org/docs/stable/nn.functional.html#cosine-similarity) to calculate the cosine similarity between two vectors x & y, as demonstrated below between two orthogonal vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7fDGUx6fBR3",
        "outputId": "58fa65e0-5757-4766-c289-72a59534357e"
      },
      "source": [
        "import torch.nn as nn\n",
        "nn.functional.cosine_similarity(\n",
        "     torch.tensor([1.0,0]),\n",
        "     torch.tensor([0,1.0],), dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "_CtZdfzNGj89",
        "outputId": "4b3b7356-3738-4ead-c99e-a6788483f101"
      },
      "source": [
        "r_df_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>movieId</th>\n",
              "      <th>m1</th>\n",
              "      <th>m10</th>\n",
              "      <th>m100</th>\n",
              "      <th>m100044</th>\n",
              "      <th>m100068</th>\n",
              "      <th>m100083</th>\n",
              "      <th>m100106</th>\n",
              "      <th>m100159</th>\n",
              "      <th>m100163</th>\n",
              "      <th>m100194</th>\n",
              "      <th>m100226</th>\n",
              "      <th>m100277</th>\n",
              "      <th>m1003</th>\n",
              "      <th>m100302</th>\n",
              "      <th>m100304</th>\n",
              "      <th>m100306</th>\n",
              "      <th>m100326</th>\n",
              "      <th>m100383</th>\n",
              "      <th>m100390</th>\n",
              "      <th>m100397</th>\n",
              "      <th>m1004</th>\n",
              "      <th>m100487</th>\n",
              "      <th>m100498</th>\n",
              "      <th>m1005</th>\n",
              "      <th>m100507</th>\n",
              "      <th>m100527</th>\n",
              "      <th>m100553</th>\n",
              "      <th>m100556</th>\n",
              "      <th>m100579</th>\n",
              "      <th>m1006</th>\n",
              "      <th>m100611</th>\n",
              "      <th>m1007</th>\n",
              "      <th>m100714</th>\n",
              "      <th>m100737</th>\n",
              "      <th>m1008</th>\n",
              "      <th>m100810</th>\n",
              "      <th>m100843</th>\n",
              "      <th>m100882</th>\n",
              "      <th>m1009</th>\n",
              "      <th>m100906</th>\n",
              "      <th>...</th>\n",
              "      <th>m98836</th>\n",
              "      <th>m98908</th>\n",
              "      <th>m98961</th>\n",
              "      <th>m99</th>\n",
              "      <th>m990</th>\n",
              "      <th>m99005</th>\n",
              "      <th>m99007</th>\n",
              "      <th>m99030</th>\n",
              "      <th>m99087</th>\n",
              "      <th>m991</th>\n",
              "      <th>m99106</th>\n",
              "      <th>m99112</th>\n",
              "      <th>m99114</th>\n",
              "      <th>m99117</th>\n",
              "      <th>m99122</th>\n",
              "      <th>m99130</th>\n",
              "      <th>m99145</th>\n",
              "      <th>m99149</th>\n",
              "      <th>m99191</th>\n",
              "      <th>m993</th>\n",
              "      <th>m994</th>\n",
              "      <th>m99415</th>\n",
              "      <th>m99437</th>\n",
              "      <th>m99532</th>\n",
              "      <th>m99574</th>\n",
              "      <th>m996</th>\n",
              "      <th>m99636</th>\n",
              "      <th>m99638</th>\n",
              "      <th>m99721</th>\n",
              "      <th>m99728</th>\n",
              "      <th>m99750</th>\n",
              "      <th>m99764</th>\n",
              "      <th>m998</th>\n",
              "      <th>m99813</th>\n",
              "      <th>m99846</th>\n",
              "      <th>m99853</th>\n",
              "      <th>m999</th>\n",
              "      <th>m99910</th>\n",
              "      <th>m99917</th>\n",
              "      <th>m99992</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>userId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>u1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u100</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u101</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u102</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u95</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u96</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u97</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u98</th>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u99</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>610 rows × 9724 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "movieId   m1  m10  m100  m100044  m100068  ...  m99853  m999  m99910  m99917  m99992\n",
              "userId                                     ...                                      \n",
              "u1       4.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u10      0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u100     0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u101     0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u102     0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "...      ...  ...   ...      ...      ...  ...     ...   ...     ...     ...     ...\n",
              "u95      0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u96      5.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u97      0.0  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u98      4.5  0.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "u99      0.0  4.0   0.0      0.0      0.0  ...     0.0   0.0     0.0     0.0     0.0\n",
              "\n",
              "[610 rows x 9724 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "bTOikrjhI6Se",
        "outputId": "2dc3a137-b774-473f-c6d9-c6c95c26fb41"
      },
      "source": [
        "movies_df[movies_df['title']=='What Happened, Miss Simone? (2015)']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8739</th>\n",
              "      <td>m127164</td>\n",
              "      <td>What Happened, Miss Simone? (2015)</td>\n",
              "      <td>Documentary</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      movieId                               title       genres\n",
              "8739  m127164  What Happened, Miss Simone? (2015)  Documentary"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZRjOLYetSgCZ",
        "outputId": "0e5067b4-6d87-4b09-be6c-bdc357d3b4df"
      },
      "source": [
        "getMovieTitle(516)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'City Hall (1996)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rg_DuBnoMEu",
        "outputId": "9b706ee2-2dfe-4f41-da3e-f1e619905cf2"
      },
      "source": [
        "def mostsimilar(targetIId : int, model):\n",
        "  highest=0\n",
        "  highestCos=0\n",
        "  \n",
        "  #you may assume that model._num_items provides the total number of items\n",
        "  x=[]\n",
        "  y=[]\n",
        "  for i in r_df_matrix:\n",
        "    #print(i)\n",
        "    ids=iid_map[i]\n",
        "    x.append(ids)\n",
        "    #print(ids)\n",
        "    cos_sim=nn.functional.cosine_similarity(\n",
        "     emodel._net.item_embeddings.weight[ids],\n",
        "     emodel._net.item_embeddings.weight[targetIId], dim=0)\n",
        "    y.append(cos_sim.detach().numpy())\n",
        "    #print(cos_sim)\n",
        "  print(\"max\",max(y))\n",
        "  y_sort=np.argsort(y)[::-1]\n",
        "  highestCos=y[y_sort[1]]\n",
        "  highest=x[y_sort[1]]\n",
        "\n",
        "  \n",
        "\n",
        "  print(\"targetMovieId = %s '%s' (iid %d)\" % (iid_rev_map.get(targetIId), getMovieTitle(targetIId), targetIId))\n",
        "  print(\"mostSimilar = %s %s with cosine of %f \" % (iid_rev_map.get(highest), getMovieTitle(highest),highestCos))\n",
        "\n",
        "\n",
        "\n",
        "mostsimilar(iid_map[\"m81834\"], emodel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max 1.0\n",
            "targetMovieId = m81834 'Harry Potter and the Deathly Hallows: Part 1 (2010)' (iid 1933)\n",
            "mostSimilar = m69844 Harry Potter and the Half-Blood Prince (2009) with cosine of 0.793590 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Br4nChVqUAj"
      },
      "source": [
        "Hopefully, you can see a correspondence between the nearest movie to `\"m81834\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgUvFR0AK8XI",
        "outputId": "8df53ab4-0695-420d-d449-204d9b0a4670"
      },
      "source": [
        "mostsimilar(iid_map[\"m88125\"], emodel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max 1.0\n",
            "targetMovieId = m88125 'Harry Potter and the Deathly Hallows: Part 2 (2011)' (iid 1938)\n",
            "mostSimilar = m69844 Harry Potter and the Half-Blood Prince (2009) with cosine of 0.765978 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENhT5FXX3qH9",
        "outputId": "6855770f-0318-4b2a-dcb0-2eaabfcd19a4"
      },
      "source": [
        "mostsimilar(iid_map[\"m44\"], emodel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max 1.0\n",
            "targetMovieId = m44 'Mortal Kombat (1995)' (iid 971)\n",
            "mostSimilar = m107338 Dampfnudelblues (2013) with cosine of 0.641703 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxHfMZgbcdRu"
      },
      "source": [
        "## Evaluating performance\n",
        "\n",
        "Finally, let's see how good we are at our rating predictions. Handily, Spotlight implements a few common evaluation measures for us to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB8UJykycm3G",
        "outputId": "0403c3ba-5779-4035-cebb-730ccd54493a"
      },
      "source": [
        "from spotlight.evaluation import rmse_score\n",
        "\n",
        "train_rmse = rmse_score(emodel, train)\n",
        "test_rmse = rmse_score(emodel, test)\n",
        "\n",
        "print('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train RMSE 0.421, test RMSE 1.078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcRrTWx9lkoo"
      },
      "source": [
        "## Task 5. Tuning\n",
        "\n",
        "It's appropriate to tune the latent factors. Normally we would use a held-out *validation* for setting the parameters, but as an exercise it is useful to examine performance on the training and test data.\n",
        "\n",
        "The task here is to train and evaluate new instances of ExplicitFactorizationModels using different numbers of latent factors, while leaving the other parameters unchanged (i.e. `n_iter=10, use_cuda=False, random_state=np.random.RandomState(SEED)`. \n",
        "\n",
        "You should also record the training times for different numbers of latent factors.\n",
        "\n",
        "You should vary the factors in `[8,16,32,64]`. Evaluate and record the RMSE values of the resulting models on both the training and test sets. Use matplotlib to create a graph showing how training and test RMSE change as the number of latent factors is varied. Use [plt.savefig()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html) to save a PNG of your graph.\n",
        "\n",
        "You can now answer the questions about Task 5 in the quiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42zBfjCEL6pI",
        "outputId": "bd390cac-3681-405b-872b-fba7516a7d95"
      },
      "source": [
        "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
        "import time  \n",
        "from spotlight.evaluation import rmse_score\n",
        "\n",
        "dim=[8,16,32,64]\n",
        "train_rmse=[]\n",
        "test_rmse=[]\n",
        "\n",
        "for i in dim:\n",
        "  emodel_2 = ExplicitFactorizationModel(n_iter=10,\n",
        "                                    embedding_dim=i, #this is Spotlight default\n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED)) # ensure results are repeatable)\n",
        "  emodel_2.fit(train,verbose=True)\n",
        "  train_rmse.append(rmse_score(emodel_2, train))\n",
        "  test_rmse.append(rmse_score(emodel_2, test))\n",
        "print(train_rmse)\n",
        "print(test_rmse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 5.744367653810525\n",
            "Epoch 1: loss 0.9479460210739812\n",
            "Epoch 2: loss 0.6766490359095079\n",
            "Epoch 3: loss 0.571296705852581\n",
            "Epoch 4: loss 0.5091710763075684\n",
            "Epoch 5: loss 0.4707542309844041\n",
            "Epoch 6: loss 0.44219849220936813\n",
            "Epoch 7: loss 0.42137832270015646\n",
            "Epoch 8: loss 0.40568300855310657\n",
            "Epoch 9: loss 0.39465059566346905\n",
            "Epoch 0: loss 4.747190550724162\n",
            "Epoch 1: loss 0.871105861060227\n",
            "Epoch 2: loss 0.6259694280503671\n",
            "Epoch 3: loss 0.496191329028033\n",
            "Epoch 4: loss 0.41484170623972444\n",
            "Epoch 5: loss 0.3629063280605817\n",
            "Epoch 6: loss 0.33110926833145227\n",
            "Epoch 7: loss 0.3086943342055701\n",
            "Epoch 8: loss 0.29521984041114396\n",
            "Epoch 9: loss 0.28318325123643573\n",
            "Epoch 0: loss 4.3081090674747395\n",
            "Epoch 1: loss 0.8099101063194154\n",
            "Epoch 2: loss 0.5096786571077153\n",
            "Epoch 3: loss 0.3636633798102789\n",
            "Epoch 4: loss 0.2919712789073775\n",
            "Epoch 5: loss 0.25697739233699024\n",
            "Epoch 6: loss 0.23643478482395788\n",
            "Epoch 7: loss 0.22271784451566165\n",
            "Epoch 8: loss 0.2139979781983774\n",
            "Epoch 9: loss 0.20728877597028697\n",
            "Epoch 0: loss 3.9413014075046853\n",
            "Epoch 1: loss 0.7578929977703698\n",
            "Epoch 2: loss 0.4151066004098216\n",
            "Epoch 3: loss 0.28894888586069967\n",
            "Epoch 4: loss 0.2509215380289132\n",
            "Epoch 5: loss 0.24193848245128802\n",
            "Epoch 6: loss 0.24069374092394794\n",
            "Epoch 7: loss 0.23878417027336132\n",
            "Epoch 8: loss 0.23337350988501235\n",
            "Epoch 9: loss 0.22632363943170897\n",
            "[0.57729393, 0.48349097, 0.42138827, 0.46829987]\n",
            "[1.010825, 1.047683, 1.0784731, 1.0378313]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "401bkqBuY_ri",
        "outputId": "0af7eb8c-bfcb-42b3-f0c4-63e64764ad53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(dim, train_rmse, label = \"Train_RMSE\",marker=\"x\")\n",
        "plt.plot(dim, test_rmse, label = \"Test_RMSE\",marker=\"x\")\n",
        "plt.xlabel('No. of factors')\n",
        "plt.ylabel('Loss value')\n",
        "plt.title('Train and Test RMSE with varying no. of factors') \n",
        "plt.legend() \n",
        "plt.savefig('train_test_rmse.png',bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU9f348dc7NwmBAOEOR0RAECFgvFARtSpqPWqtINiC+pVaTzy+fj1qpX7Lr/bb2nq2aK3ijVZbRetVlEtRSzgFFLkhnCGQgyPkev/+mNlks9lNNsdms9n38/GYR+aez2x25z3z/sx8RlQVY4wx0Ssm3AUwxhgTXhYIjDEmylkgMMaYKGeBwBhjopwFAmOMiXIWCIwxJspZIGgBIvKhiExuBeWYLiKvhLsckU5EzhSRdXVM7y8iKiJxLVmuYIjIGhEZG+5yNBcRGSwiK0SkWERu8zO9u4gsdKc/Go4yRgILBAGIyEGvrlJEjngNT2rIulT1QlV9MVRlbSoRmeS1b0fc/a3a/0asr94DoRuUytxtFIjIYhE5zWv6WHcd//RZboQ7fr7XuMvcg0GRiOwTkc9EJNPPdjxdQUP3yZuqLlLVwV7b3yIiP2jKOluKqh6vqvPDXY5mdA8wT1VTVfUJP9OnAvuADqp6V2M30tZPoiwQBKCq7T0dsA24xGvcq575WuNZX0Op6qte+3ohsNNn/0PlDXf96cA84O8+0/OA00Ski9e4ycD3ngERORZ4CbgL6AhkAk8DFb7b8erSmn9Xwq8tfBcboR+wpp7pazXMT8629v+NBYIGcs9Uc0Xkf0RkN/CCiHQSkfdFJE9EDrj9GV7LzBeR/3L7p4jI5yLyB3fezSJyYR3bu1dENrqXtmtF5Ede0+pcl4hkisgCd9l/4xxwG7q/vUTkbXffNntffovIySKS456J7xGRP7qTFrp/C9wz8NNqr7maqpYDrwK9RaSr16RS4B1ggru9WGC8O69HFrBZVT9VR7Gqvq2q2xqxry+KyF1uf2/3yuNmd3iAiOwXkRjPd8Ad/zLQF3jP3dd7vFY5SUS2uVcpDwTY5ikistvdN8+4H4nIKrf/ZBH50r1q2iUiT4lIgte8KiI3i8h6YL2IPC0+KRARmSMid7j9VVcv7lnumyLykvsdWSMi2V7LjRKR5e60v4vIGyLymwD7Ud93sZdbjv0iskFEbgjmf+Iue6lbtgL3tzTEHf8ZcDbwlPvZD/JZbhbOicM97vQfBPF5Hi8i/3bLuUdE7heRccD9wHh3PSvr2yf3s31LRF4RkSJgSh2/l/BTVevq6YAtwA/c/rFAOfA7IBFoB3QBfgwkA6k4Z7bveC0/H/gvt38KUAbcAMQCvwB2AhJg2z8BeuEE7fHAIaBnMOsCvgT+6JZzDFAMvFLPvo4Fct3+GGAp8CsgATgG2ARc4LX+n7r97YFT3f7+gAJxdWxnuqcs7rofwbmEj/MuBzAa+NoddxHwMfBfwHx33DFACfAnnINC+0DbCeL/fB3wnts/EdiIczXhmfau72fk+/3w2f+/ut+PEcBRYEiA7W4EzvMa/jtwr9t/InAqEOeu91tgmte8Cvwb6Oxu62T3OxDjTk8HDgPd/XyXp7uf3UXu9+e3wFde/5OtwO1APHAFTmD+TYB9mELd38WFwJ+BJJzgnQecE8T/ZBDOd/48txz3ABuABN/fVoDlZ3mXua7PE+e3uwvn6jLJHT4l0Peorn1y5y8DLsf5HbUjwO+lNXRhL0AkdNQOBKVAUh3zZwEHvIarvqzuD2aD17Rk98fcI8iyrAAuq29dOGep5UCK1/TXfL/MftY/lupAcAqwzWf6fcALbv9C4NdAus88/QkuEJQCBThpnHxgbIByrAcGA7OBSXgFAnf6qcCb7g+xxP3xt/ezHU83L0CZBgAH3B/uTODnXmV4EbjTt2y+3w+f/c/wGvcfYEKA7f4GeN7tT8U58PULMO804J9ew4rPARXn4Hae238L8EGA7/J0YK7XtKHAEbd/DLADrxMU4HPqDgSBvot93P9xqtf03wKzgvi+Pwi86TUc45ZrrO9vK8DyswKV2ffzBK4GltfxfX3Fa7jOfXLnX+izDr+/l9bQWWqocfJUtcQzICLJIvKMiGx1LwMXAmnel/s+dnt6VPWw2+s3Fy8iPxOnIrRAnErOYdRM8QRaVy+cYHTIa96tQe6fRz+gl2fb7vbvB7q706/HOWP7TkSWiMgPG7j+N9XJ13cHVuOcrfnzMs4B7Wzgn74TVfUrVb1KVbsCZ+IcxLxTMW+qappXd7a/jajqRpyDcJa7nveBnSIyGDgLWNDA/dvt1X+YAP9jnAB9hYgk4px5L1PVrQAiMkicVONu97v1/6id4tvuM/wicI3bfw3O5xdsGZPEyWf3AnaoewQLsJ2A6/LzXdyvqsVe824FetezPtxlq763qlrpliOYZWup5/Psg3N1Foxg9sn382rq7yVkLBA0jm/F0104Z6ynqGoHnAMRgDRlIyLSDye9cAvQxT1org5yvbuATiKS4jWubwOLsB0n/+59EE1V1YsAVHW9ql4NdMNJlb3lbq9BFXOqug/n7o7pItLTzywvAzfhnNke9jPde11LgH/gBMzGWABciZN62OEOTwY64VyN+d1sI7flLKy6FucgciFOSuo1r8l/Ab4DBrrfrfup/f/33f4rwGUiMgIYglPP0lC7cOpsvLfVpxHrASdF1FlEUr3G9cU5sw9m2X6eAbc8fYJc1p+6Ps/tOKlGf3w/42D2qcYydfxews4CQfNIBY7gVI52Bh5qpvV6Dqp5ACJyLUEe4Nwzyhzg1yKSICJnAJc0cPv/AYrFqRhvJyKxIjJMRE5yy3ONiHR1z9I8t2RWuuWtJPCPyl951+Hk/+/xM20zzhl5rQpXETlDRG4QkW7u8HHApcBXDdlRLwtwAq+nwnu+O/y5qlYEWGYPDdjXAF7DycePoebdU6lAEXDQ3bdf1LciVc0FluAE0LdV9UgjyvMlTurjFhGJE5HLcOofGkxVtwOLgd+KSJKIDMc5Ow7mdsw3gYtF5FwRicc56Trqrq8x6vo83wd6isg0EUkUkVQROcWdtgfoLyIxjd2nOn4vYWeBoHk8hlMZtA/nAPRRc6zUPVN8FOdHuQc4AfiiAauYiJPn348TnF5q4PYrgB/i3pmDs3/P4dymCTAOWCPOswaP4+TAj7hn7TOAL9yU0qlBbvL3wFTPQd2nLJ+r6k4/yxTgHPi/ccvxEU766P+85vHc7eHd1dqGawHOwcITCD7HyXcvDDA/OLnhX7r7enddO1iH13GC3WfuFZLH3Tj/x2Kcq8M3glzfizjfl7rSQgGpailOmup6nM/4GpwD5dHGrA8n/94f50z6n8BDqjoXQERmisjMAOVY5277SZzv3yU4t3KXNrIcAT9PN81znruN3Th1U540oic454vIsvr2KQC/v5dG7kez8tToG2PaEBEZg3N22k+b6UcuIl8DM1X1heZYn2k97IrAmDbGTaHcDjzXlCAgImeJSA83NTQZGE4zXe2a1qVVP+1mjGkYcR62ygFWAtc2cXWDcXL0KTjPj1ypqruauE7TCllqyBhjopylhowxJspFXGooPT1d+/fvH+5iGGNMRFm6dOk+96HLWkIWCETkeZxbD/eqaq173917eF8ARgEPqOofgllv//79ycnJadayGmNMWyciAVsWCGVqaBbOfbOB7AduA4IKAMYYY0IjZIFAVRfiHOwDTd/rNgdQFqoyGGOMqV9EVBaLyFS3He+cvLy8cBfHGGPalIgIBKr6rKpmq2p2165+6zqMMcY0UkQEAmOMMaFjgcBEj88fg80+bcdtXuiMNyaKhSwQiMjrOK1mDhbnHb/Xi8iNInKjO72HOO99vROn5cZcEekQqvIYQ+9R8Pcp1cFg80JnuPeocJbKmLAL2XME7gsY6pq+G8ioax5jmsXRYijaBZUVMPKn8Np4yMiGHUth9K3OPLu/gXadnC4+GaRJ7xQyJqJE3JPFxlSprIRDeVC80znQF++EIj/9pcW1l/VcFcx/pPa02ITqoFCrSws8LbGDBRATkSwQmNaprASKdzldkXtQL94FRTvcA73bVZbXXE5iIbUHpPaEroNhwDlOf4dezt+iHfDRfTBqMiybBT/4NXQ+Bo4cCNwVbIddq5z+skN+i1u17boCRaAuqSPEBHq9tTGhZ4HAtCxVKCnwOXN3D/DFu6r7j/h5FjE+BTq4B/V+pzv9qb2cYU9/+26BD6qbF8LH98NVL0LmGDj2HKeO4CezYOilwZW//CgcKag7cHi6g3shb50z/9HCuteb1LERASQN4hKCK7cxdbBAYJpPRTkc3ONzFu850Hv1l/t5O19KV+eMvWNv6HOSe4Dv6Z7N93b6m5p62bHMOehnjnGGM8c4wzuWVY+rT1wipHZ3uoaoKIeSwuACyJEDcGCr87ekALSO19omtK8/ZeWvi2/XsPKbNi3i3keQnZ2t1uhcGJQe8jlz907XuH8P7ql90IpNcFI1HXrXTNF06FXdn9rDOcCa2ior4WhRgIBRz5VJZR2tt8QlNaz+w9MltLd6kAglIktVNdvfNLsiiHaVlXA4v/4KV3+pjaSO1Wfu3YbWTNF4DvTJXezA0RQxMe4BOg3IDH45VSd4+waHEn/BowAObIGd7nDZ4TrKExfkVYdPUEns6OyLaZUsELRl5aVeFa5elaxV/TuheDdUlNZcTmKgfXfnQN7lWCdt4p2i8Rz8E1LCs1+mfiKQ2N7p0vo0bNmykgABw09XvAv2rnXrQYrqKlBwVxxJvvOkQWx8kz4KUz8LBK3N5485Dzh556w3L3Ty2GdMc4ZVnR9djfSMn7TN4X211x+fXJ2a6XNqzRSNpz+lG8TaVyNqxSdBfA8nZdcQFWUNqwfZv6n6ioQ6UtQJqY2sB0lq0scQTezX3tp4nn4972Hn7GjzQlj2EvQ9FTbMrT7o+7uNMblLdVqm96jad9R06Oms01I1JhRi4yEl3ekaorLSST0GW/+xd61XPUh54PXGtWt4HUi7Ts6VbpT9RiwQtAaqkL8RtiyEzYucL/e7N1dPl1hneoee0H0YDLzA644arzN6q3A1kSgmpvog3BCqUHowiKuPgppXIIf3Q8XROsoT37D6D+8HCkNRDxJMlqCJLBCEy4EtzkF/yyLnb/FOZ3xqLxg0zrnE/v4jOPVmOP83VtFmjC8RSEx1urS+DVu27EjwKayiXNiz2ukvPVhHeWL81HEE+UBhXalYT5bAc+uzp42sn8xq2D7XwQJBSyncUX3Q37IQCrY541O6Qv8zIfNMyDzLecp1yyLnHz3mHsj5GwweF/x97saY+sW3c7oOvRq2XHlp8BXph/dB/nr3bq16HihM7Fh36irrGpg9EYZPgDX/qPk8TDOwQBAqB/c6kdtz8N+/0RnfrhP0PwNOu9X5R3YdXDMf6R3tM8c4AcJ72BgTPnEJztPr7bs1bLnKioZVpBdur+73fjZnyV+dE8RmPhZYIGguh/d7nfEvgrzvnPGJHZzmEE663jnz7z6s7jRPczz9aoxpXWJiIbmz0zVEZaXTaOL3H8MHdztXBDl/czMIdkUQfiWFsHWxcwa/eZGTQ0Sd9nD6nQYjrnb+WT1GNOxWTH+VP5ljLAgYE41iYmDXSvjoXhj/inMcGPLDZs8SWCAI1tGDsO0r2LzAOePftdK5ZItLgj4nwzkPQP8xTsWOPQBjjGkuLZAlsLaGAik7Atu/ds72Ny+Encuc2zpj4iHjpOr8fe9se3DFGNPqWVtDwSg/Crk5bp5/IeQucZpekFjnLP/0250cf59TICE53KU1xphm0/YDQaCHMXKXOAf2zQudbvt/3OaRBXqOgFN+7tzO2fdU5z5lY4xpo9p+IPA8jPHj5yGpAyx7EZa/6tTif/qwM0/3YXDiFCfV0290w59wNMaYCNb2A4GnYuX1q6ufCuzQBwZf4FwR9D+j4W2jGGNMGxKydgtE5HkR2SsiqwNMFxF5QkQ2iMgqERkVqrKQOQZG/czpP/UmuHM1XPwoHH+5BQFjTNQLZQM2s4BxdUy/EBjodlOBv4SsJJsXwqo3nCfyVr3hDBtjjAFCGAhUdSHg5w3kVS4DXlLHV0CaiPRs9oJ4N9lwzgPO379PsWBgjDGucDZp2RvY7jWc646rRUSmikiOiOTk5eU1bCt1PYxhjDEmMiqLVfVZ4FlwHihr0MLWZIMxxtQpnFcEOwDvl6lmuOOMMca0oHAGgjnAz9y7h04FClV1VxjLY4wxUSlkqSEReR0YC6SLSC7wEBAPoKozgQ+Ai4ANwGHg2lCVxRhjTGAhCwSqenU90xW4ua55jDHGhJ69CNcYY6KcBQJjjIlyFgiMMSbKWSAwxpgoZ4HAGGOinAUCY4yJchYIjDEmylkgMMaYKGeBwBhjopwFAmOMiXIWCIwxJspZIDDGmChngcAYY6KcBQJjjIlyFgiMMSbKWSAwxpgoZ4HAGGOinAUCY4yJchYIjDEmylkgMMaYKGeBwBhjolxIA4GIjBORdSKyQUTu9TO9n4h8KiKrRGS+iGSEsjzGGGNqC1kgEJFY4GngQmAocLWIDPWZ7Q/AS6o6HHgY+G2oymOMMca/UF4RnAxsUNVNqloKzAYu85lnKPCZ2z/Pz3RjjDEhFspA0BvY7jWc647zthK4wu3/EZAqIl18VyQiU0UkR0Ry8vLyQlJYY4yJVuGuLL4bOEtElgNnATuACt+ZVPVZVc1W1eyuXbu2dBmNMaZNiwvhuncAfbyGM9xxVVR1J+4VgYi0B36sqgUhLJMxxhgfobwiWAIMFJFMEUkAJgBzvGcQkXQR8ZThPuD5EJbHGGOMHyELBKpaDtwCfAx8C7ypqmtE5GERudSdbSywTkS+B7oDM0JVHmOMMf6Jqoa7DA2SnZ2tOTk54S6GMcZEFBFZqqrZ/qaFu7LYGGNMmFkgMMaYKGeBwBhjopwFAmOMiXIWCIwxJspZIDDGmChngcAYY6KcBQJjjIlyFgiMMSbKWSAwxpgoZ4HAGGOinAUCY4yJchYIjDEmylkgMMaYKGeBwBhjoly9gUBEuovI30TkQ3d4qIhcH/qiGWOMaQnBXBHMwnnLWC93+HtgWqgKZIwxpmUFEwjSVfVNoBKqXkFZEdJSGWOMaTHBBIJDItIFUAARORUoDGmpjDHGtJi4IOa5E5gDDBCRL4CuwJUhLZUxxpgWU28gUNVlInIWMBgQYJ2qloW8ZMYYY1pEvYFARH7mM2qUiKCqLwWx7DjgcSAWeE5VH/GZ3hd4EUhz57lXVT8ItvDGGGOaLpjU0Ele/UnAucAyoM5AICKxwNPAeUAusERE5qjqWq/Zfgm8qap/EZGhwAdA/+CLb4xp7crKysjNzaWkpCTcRYkKSUlJZGRkEB8fH/QywaSGbvUeFpE0YHYQ6z4Z2KCqm9zlZgOXAd6BQIEObn9HYGcQ6zXGRJDc3FxSU1Pp378/IhLu4rRpqkp+fj65ublkZmYGvVxjniw+BASzhd7Adq/hXHect+nANSKSi3M1cCt+iMhUEckRkZy8vLyGl9gYEzYlJSV06dLFgkALEBG6dOnS4KuvYOoI3sO9dRQncAwF3mxwCf27Gpilqo+KyGnAyyIyTFUrvWdS1WeBZwGys7PVz3qMMa2YBYGW05jPOpg6gj949ZcDW1U1N4jldgB9vIYz3HHergfGAajqlyKSBKQDe4NYvzHGmGZQb2pIVRd4dV8EGQQAlgADRSRTRBKACTjPI3jbhlP5jIgMwamMttyPMVFq5oKNLN64r8a4xRv3MXPBxkavMz8/n6ysLLKysujRowe9e/euGi4tLa1z2ZycHG677bZGbTc2NpasrCyGDRvGJZdcQkFBAQBbtmxBRPjlL39ZNe++ffuIj4/nlltuAWDdunWMHTuWrKwshgwZwtSpUwGYP38+HTt2rCp/VlYWc+fObVT5alBVvx1QDBT56YqBokDL+azjIpy2iTYCD7jjHgYudfuHAl8AK4EVwPn1rfPEE09UY0zkWLt2bdDzfrEhT0c+/Il+sSHP73BTPfTQQ/r73/++xriysrJmWbevlJSUqv6f/exn+pvf/EZVVTdv3qyZmZmalZVVNf3Pf/6zjhgxQm+++WZVVT3//PP1nXfeqZq+atUqVVWdN2+eXnzxxfVu299nDuRogONqwNSQqqY2Q5D5AKcS2Hvcr7z61wKnN3U7xpjI8Ov31rB2Z1Gd83RLTeRnf/sP3TsksqfoKMd2a8/jc9fz+Nz1fucf2qsDD11yfIPKMWXKFJKSkli+fDmnn346EyZM4Pbbb6ekpIR27drxwgsvMHjwYObPn88f/vAH3n//faZPn862bdvYtGkT27ZtY9q0aUFfLZx22mmsWrWqajg5OZkhQ4aQk5NDdnY2b7zxBldddRU7dzo3Tu7atYuMjIyq+U844YQG7V9DBVNHAICIdMNJ3QCgqttCUiJjTFTr2C6e7h0S2VFQQu+0JDq2C/5++IbIzc1l8eLFxMbGUlRUxKJFi4iLi2Pu3Lncf//9vP3227WW+e6775g3bx7FxcUMHjyYX/ziF/Xer19RUcGnn37K9dfXbL1/woQJzJ49m+7duxMbG0uvXr2qAsEdd9zBOeecw+jRozn//PO59tprSUtLA2DRokVkZWVVreftt99mwIABTfosgrlr6FLgUZxmqPcC/YBvgYaFYGNM1AvmzH3xxn3c8tpybjvnWF75ehu3/2AgowekN3tZfvKTnxAbGwtAYWEhkydPZv369YgIZWX+W9G5+OKLSUxMJDExkW7durFnz54aZ+7ejhw5QlZWFjt27GDIkCGcd955NaaPGzeOBx98kO7duzN+/Pga06699louuOACPvroI959912eeeYZVq5cCcCZZ57J+++/39TdryGY5wj+FzgV+F5VM3Eqd79q1lIYYwzVQeCpiSO58/zBPDVxJLe8trxWBXJzSElJqep/8MEHOfvss1m9ejXvvfdewPvwExMTq/pjY2MpLy8PuP527dqxYsUKtm7diqry9NNP15iekJDAiSeeyKOPPsqVV9Zux7NXr15cd911vPvuu8TFxbF69eqG7mLQggkEZaqaD8SISIyqzgOyQ1YiY0zUWpVbyFMTR1ZdAYwekM5TE0eyKje0Ld8XFhbSu7fzvOusWbOadd3Jyck88cQTPProo7UCx1133cXvfvc7OnfuXGP8Rx99VHVVsnv3bvLz86vKFwrBBIICEWkPLAReFZHHcZ4uNsaYZnXjWQNqpYFGD0jnxrOalgOvzz333MN9993HyJEj6zzLb6yRI0cyfPhwXn/99Rrjjz/+eCZPnlxr/k8++YRhw4YxYsQILrjgAn7/+9/To0cPoLqOwNO99dZbTS6fOHcV1TGDSApwBCdoTMJpE+hV9yqhxWVnZ2tOTk44Nm2MaYRvv/2WIUOGhLsYUcXfZy4iS1XVbzYnmLuGfg68oao7cJqMNsYY04YEEwhSgU9EZD/wBvB3Vd0T2mIZY0zrl5+fz7nnnltr/KeffkqXLl3CUKLGCaYZ6l8DvxaR4cB4YIGI5KrqD0JeOmOMacW6dOnCihUrwl2MJmtIM9R7gd1APtAtNMUxxhjT0uoNBCJyk4jMBz4FugA3qOrwUBfMGGNMywimjqAPME1VI//6xxhjTC3B1BHc1xIFMcYYEx6NeVWlMcaExuePweaFNcdtXuiMb6SmvI8AnHcALF68uM55pk+fXrXeoUOH1nhwbMqUKSQnJ1NcXFw1btq0aYgI+/Y5TWfMmDGD448/nuHDh5OVlcXXX38NwNixYxk8eHBVef01RdEcgm591BhjQq73KPj7FPjJLMgc4wQBz3Ajed/ZM336dNq3b8/dd98d9PLz58+nffv2jB49us757rjjDu6++27Wr1/PiSeeyJVXXlnVMumxxx7Lu+++yzXXXENlZSWfffZZVZMRX375Je+//z7Lli0jMTGRffv21QhQr776KtnZoW3VJ5jWR1OAI6paKSKDgOOAD1XVf/N8xhgTyIf3wu5v6p4ntSe8/CPnb/Eu6HoczP+d0/nT4wS48JEGFWPp0qXceeedHDx4kPT0dGbNmkXPnj154oknmDlzJnFxcQwdOpRHHnmEmTNnEhsbyyuvvMKTTz7JmWeeWee6Bw4cSHJyMgcOHKBbN+cGywkTJvDGG29wzTXXMH/+fE4//XQ+/PBDwHn3QHp6elWDdunpzd/San2CSQ0tBJJEpDfwCfBTYFYoC2WMiWJJaU4QKNzu/E1Ka9bVqyq33norb731FkuXLuW6667jgQceAOCRRx5h+fLlrFq1ipkzZ9K/f39uvPFG7rjjDlasWFFvEABYtmwZAwcOrAoCAIMGDSIvL48DBw7w+uuvM2HChKpp559/Ptu3b2fQoEHcdNNNLFiwoMb6Jk2aVJUa+u///u9m+hRqCiY1JKp6WESuB/6sqv8nInYHkTGm4YI5c/ekg8bcAzl/g7H/46SJmsnRo0dZvXp11fsBKioq6NmzJwDDhw9n0qRJXH755Vx++eUNWu+f/vQnXnjhBb7//nvee++9WtOvuOIKZs+ezddff80zzzxTNb59+/YsXbqURYsWMW/ePMaPH88jjzzClClTgFaSGgJERE7DaXDO84qd2NAVyRgTtbzrBDLHQOaZNYebgapy/PHH8+WXX9aa9q9//YuFCxfy3nvvMWPGDL75pp40lhdPHcGcOXO4/vrr2bhxI0lJVS91ZPz48Zx44olMnjyZmJiayZjY2FjGjh3L2LFjOeGEE3jxxRerAkFLCCY1NA24D/inqq4RkWOAeaEtljEmKu1YVvOgnznGGd6xrNk2kZiYSF5eXlUgKCsrY82aNVRWVrJ9+3bOPvtsfve731FYWMjBgwdJTU2tccdPfS699FKys7N58cWabXT269ePGTNmcNNNN9UYv27dOtavr34f84oVK+jXr18T9rDhgnmOYAGwAEBEYoB9qhrcG5uNMaYhzphWe1zmmGZNDcXExPDWW29x2223UVhYSHl5OdOmTWPQoEFcc801FBYWoqrcdtttpKWlcckll3DllVfy7rvvBlVZDPCrX/2KiRMncsMNN9QY//Of/7zWvD1FBOIAABdPSURBVAcPHuTWW2+loKCAuLg4jj32WJ599tmq6ZMmTaJdu3aAU5E8d+7cJn4CtQXzPoLXgBuBCmAJ0AF4XFV/X+/KRcYBj+Okkp5T1Ud8pv8JONsdTAa6qWqdNUP2PgJjIou9j6DlNfR9BMGkhoaqahFwOfAhkIlz51CdRCQWeBq4EBgKXC0iQ73nUdU7VDVLVbOAJ4F/BFEeY4wxzSiYQBAvIvE4gWCO+/xA3ZcRjpOBDaq6SVVLgdnAZXXMfzXweh3TjTEmrGbMmFHjNZFZWVnMmDEj3MVqsmDuGnoG2AKsBBaKSD+gKIjlegPbvYZzgVP8zeiuMxP4LMD0qcBUgL59+waxaWNMa6KqiEi4i9FkDzzwQNUzB61Vfel+f+q9IlDVJ1S1t6pepI6tVOf1m8sE4C1VrQhQhmdVNVtVs7t27drMmzbGhFJSUhL5+fmNOkCZhlFV8vPza9y2GoxgmpjoCDwEeKrtFwAPA4X1LLoDpwlrjwx3nD8TgJvrK4sxJvJkZGSQm5tLXl5euIsSFZKSksjIyGjQMsGkhp4HVgNXucM/BV4ArqhnuSXAQBHJxAkAE4CJvjOJyHFAJ6D20x3GmIgXHx9PZmZmuIth6hBMIBigqj/2Gv51ME1MqGq5iNwCfIxz++jz7gNpDwM5qjrHnXUCMFvtutEYY8IimEBwRETOUNXPAUTkdOBIMCtX1Q+AD3zG/cpneHpwRTXGGBMKwQSCG4GX3LoCgAPA5NAVyRhjTEsKpomJlcAIEengDheJyDRgVagLZ4wxJvSCflWlqha5TxgD3Bmi8hhjjGlhjX1nceQ/GWKMMQZofCCwO3yMMaaNCFhHICLF+D/gC9AuZCUyxhjTogIGAlVNbcmCGGOMCY/GpoaMMca0ERYIjDEmylkgMMaYKGeBwBhjopwFAmOMiXIWCIwxJspZIDDGmChngcAYY6KcBQJjjIlyFgiMMSbKWSAwxpgoZ4HAGGOinAUCY4yJchYIjDEmyoU0EIjIOBFZJyIbROTeAPNcJSJrRWSNiLwWyvIYY4yprd6X1zeWiMQCTwPnAbnAEhGZo6prveYZCNwHnK6qB0SkW6jKY4wxxr9QXhGcDGxQ1U2qWgrMBi7zmecG4GlVPQCgqntDWB5jjDF+hDIQ9Aa2ew3nuuO8DQIGicgXIvKViIzztyIRmSoiOSKSk5eXF6LiGmNMdAp3ZXEcMBAYC1wN/FVE0nxnUtVnVTVbVbO7du3awkU0xpi2LZSBYAfQx2s4wx3nLReYo6plqroZ+B4nMBhjjGkhoQwES4CBIpIpIgnABGCOzzzv4FwNICLpOKmiTSEskzHGGB8hCwSqWg7cAnwMfAu8qaprRORhEbnUne1jIF9E1gLzgP9W1fzmLMfMBRtZvHFfjXGLN+5j5oKNzbkZY4yJWCGtI1DVD1R1kKoOUNUZ7rhfqeoct19V9U5VHaqqJ6jq7OYuw/CMjtzy2nIWb9yHqrJ44z5ueW05wzM6NvemjDEmIoXsOYLWYvSAdJ6aOJJfvLKMpPgYjpRWMPOnJzJ6QHq4i2aMMa1CuO8aahGjB6RzznHd2FN0lOKSchZ+v4+SsopwF8sYY1qFqAgEizfuY8H3eUw98xgS4mKYuWAjFz6+iK83NWt1hDHGRKQ2Hwg8dQJPTRzJ/RcP4YVrTyI1KY7ikjLGP/sVD76zmuKSsnAX0xhjwqbN1xGsyi3kqYkjq+oERg9I55mfnsjSLQc4cLiMFxZv5tNv9zDjRydw9nHW1JExJvqIqoa7DA2SnZ2tOTk5zba+ZdsO8D9vrWL93oP8aGRvHvzhUDqnJDTb+o0xpjUQkaWqmu1vWptPDdVnVN9OvH/bGdx27kDeW7mT8/64gPdW7iTSAqQxxjRW1AcCgMS4WO48bxDv3XoGvTu149bXl3PDS0vZU1QS7qIZY0zIWSDwMqRnB/7xi9Hcf9FxLFqfxw/+uIDZ/9lmVwfGmDbNAoGPuNgYpo4ZwMfTxjC0Zwfu/cc3THrua7bmHwp30YwxJiQsEATQPz2F1284lf/3oxP4JreQCx5byHOLNlFRaVcHxpi2xQJBHWJihImn9OWTO8dw+oB0fvOvb7niL4tZt7s43EUzxphmY4EgCD07tuO5ydk8cfVItu8/zA+fXMRjc7+ntLwy3EUzxpgms0AQJBHh0hG9mHvnWVx0Qk8em7ueS578nBXbC8JdNGOMaRILBA3UOSWBxyeM5G+Tsyk8UsYVf/6C37y/liOl1oidMSYyWSBopHOHdOffd47h6pP78tznm7ngsYW1XoBjjDGRwAJBE6QmxTPjRycwe+qpxAhM/OvX3PePVRQesUbsjDGRwwJBMzj1mC58ePsYfj7mGN5Ysp3z/7SAf6/dE+5iGWNMUCwQNJN2CbHcd9EQ3rn5dDolJ3DDSznc8toy9h08Gu6iGWNMnSwQNLPhGWnMueUM7jpvEJ+s2cN5f1zAO8t3WDMVxphWywJBCCTExXDruQP5121n0D89hWlvrOC6WUvYWXAk3EUzxphaQhoIRGSciKwTkQ0icq+f6VNEJE9EVrjdf4WyPC1tYPdU3rpxNA9dMpSvNu3n/D8t5OWvtlJpzVQYY1qRkAUCEYkFngYuBIYCV4vIUD+zvqGqWW73XKjKEy6xMcK1p2fyyR1jyOqTxoPvrGbCs1+xKe9guItmjDFAaK8ITgY2qOomVS0FZgOXhXB7rVqfzsm8fP3J/N+Vw/ludxEXPr6ImQs2Ul5hzVQYY8IrlIGgN7DdazjXHefrxyKySkTeEpE+/lYkIlNFJEdEcvLy8kJR1hYhIlyV3Ye5d57F2MFdeeTD77j8z1+wdmdRuItmjIli4a4sfg/or6rDgX8DL/qbSVWfVdVsVc3u2rVrixYwFLp1SOKZn2bzl0mj2F14lEuf+pw/fLyOkjJrpsIY0/JCGQh2AN5n+BnuuCqqmq+qnhvtnwNODGF5Wp0LT+jJ3DvHcFlWb56at4GLn1jEg+98U6upisUb9zFzwcYwldIY09aFMhAsAQaKSKaIJAATgDneM4hIT6/BS4FvQ1ieViktOYFHrxrBi9edTElZJS9/tY1rX1jCZ985TyYv3riPW15bzvCMjmEuqTGmrZJQPugkIhcBjwGxwPOqOkNEHgZyVHWOiPwWJwCUA/uBX6jqd3WtMzs7W3NyckJW5nA6dLSc33+8jlmLtxAjMGZgV5ZtO8DTk0Zx5sDIT4kZY8JHRJaqarbfaZH2xGtbDgQeOVv2M/WlHPYfdhqvS4qP4YTeHcnqk0ZWn05k9U2jV8ckRCTMJTXGRIq6AkFcSxfG1K+0ohJEmHxaP95alsuZx6azp/goL365lb8u2gxA19RENzCkMbJPGsP7pNE+0f6dxpiGsyNHK+OpE3hq4khGD0jngmE9qoaz+3Xm211FrNheUNV5WjkVgYHd2ldfNfRJY1D39sTFhvvGMGNMU8xcsJHhGR0ZPSC9atzijftYlVvIjWcNaJZtWGqolWnoP73gcGmNwLByewEH3JRSu/hYTsjoyEj3yiGrbxo9O7ZrsX0xxjSd98nhacd04ctN+TVOFoNldQRRRFXZmn+4KjAs317AtzuLnHQT0L1DYo2rhuEZHUmxlJIxYVFSVkH+oVL2Hywl/9BRDhwuJf9gKfsPOV3+oVIOHCplx4Ej7C4q4aT+ndmQd7DBQQCsjiCqiAj901Pon57C5SOdB7mPllewdmdRjauGj9c4KaUYgUHdU6vqG7L6pjGwWyqxMVYRbUxDqCpFJeXuQfwo+w+Vsf/Q0aoD/f7DXgd492B/JMBDpLExQqfkBLqkJNA5JYFR/TuxLf8w/9myn9vOObbBQaA+FgiiQGJcLCP7dmJk305V4w4cKmVFbgHLtznB4cPVu5m9xGkRJCXBSSl5rhpG9k2je4ekcBXfmLAor6jkwOGyqoO35wDvOUvPP1T7zL08QMvCSfExdElJpLN7YD+2a3s6uf2eg33nquFEOrSLq3FXoCc9dNs5x/LK19s4dUCXZg0GlhoygHM2s3nfoRr1Dd/uKqKswvl+9OyYVH3V0CeNEzI6kpxg5xEmcnjSMNUH8aPkHyzlwOGaZ+meM/fCI2UEOjx2SIqjS3vnwF515t6++qDeyesA3yUlkXYJsY0ut+8NJL7DwbI6AtMoJWUVrN1VxIpt1cFh2/7DgHPp6kkpjXRTSsd2bU+MpZRMC1BVio+Wu7n1Uv/pGJ8z98Ol9adhOqXE1zhz79K+5oHec+CPb8G78ZrrriELBKbZ5B88ysrcAlZscyqiV24voKikHID2iXEMz+hYo76hW6qllEz9KirV/5n5Qa90jFdF6oHDpVVXq748aZhOKfF0TkmslXrxTcd0SIqPihMYCwQmZCorlc35h2pcNXy7q6gqV9o7rV2NwDCsV8cmXSabyFBSVlErf55fddZeM7ceTBqm+iCeWH12nuyOc1MynZKdM3hLWfpngcC0qJKyCtbsLKyqiF6xvYDcA877mmNjhON6VN+lNLJvGsekW0qpNfNOw1SfpXsf2MuqDvCeg/6hOtMw8TUqRn3P3Lt45dg7pbRsGqYts0Bgwi6v+CgrfR58Kz7qpJRSk+IYkZFW48ohvX1imEvcdnnSMN75c88B3PsA76lIPXCorOo5FF+JcTFe+fNEOie7B/X2/tMx0ZKGaY0sEJhWp7JS2bTvYI2rhu92F1PhppQyOrWrcdVwfK+OJMVbSsmfkrKKWg8i7fc5qB84VEa+e9ZeUEcaJjUpzk9OPUCevX0C7eJjrfHDCGGBwESEI6UVrN5ZWKO+YUeBk1KKixGG9OxQ46ohs0tKmzu7VFUOHi2vPoj7eRDpwGGvtMzBwGmYGKHqoO3Jn3sO7J2T4+ncvmY6Ji05gYQ4S8O0VRYITMTaW1xSIzCsyi3koJtS6pAUxwiv21dHZKTRpZWllCoqlYLD/ipNA5+515eGqfkgUiKd3Ry75yy9c4pTkdqxnaVhTDULBKbNqKhUNuYdrLp9dcX2AtbtLsLzQGffzsk1rhqG9uxQlVJqjvuxj5ZX1Dgz93+7o6fdmDIOHC6tMw3jeztjJ68DvG86JjnB0jCm8SwQmDbtcGk53+QW1ngqeldhCQDxscJQN6WUkhjHq19v4y+TRjH62HQWb9jHza8t46FLjqdfl+QatzMGqkj1XI34ihHolFwzf+7dVownDeNJ0XSyNIxpYRYITNTZU1TiVRF9gFW5hVVPlgqQnBAbMLcOkOC5G8bvg0g+6ZgUS8OY1s9aHzVRp3uHJMYN68G4YT0AJ6W0fm8xK7YV8PJXW1mzs4hRfdMYN6yHV0VqdTrG0jAmmlggMFHBeZCtA/sPlbKrsKSqFcdhvTs2e5O+xkQaS1KaqOHdauOd5w/mqYkjueW15SzeuC/cRTMmrEIaCERknIisE5ENInJvHfP9WERURPzmr4xpDqtyC2s03Tt6QDpPTRzJqtzCMJfMmPAKWWpIRGKBp4HzgFxgiYjMUdW1PvOlArcDX4eqLMYAfm8RHT0g3VJDJuqF8orgZGCDqm5S1VJgNnCZn/n+F/gdUBLCshhjjAkglIGgN7DdazjXHVdFREYBfVT1X3WtSESmikiOiOTk5eU1f0mNMSaKha2yWERigD8Cd9U3r6o+q6rZqprdtWvX0BfOGGOiSCgDwQ6gj9dwhjvOIxUYBswXkS3AqcAcqzA2xpiWFcpAsAQYKCKZIpIATADmeCaqaqGqpqtqf1XtD3wFXKqq9tiwMca0oJDdNaSq5SJyC/AxEAs8r6prRORhIEdV59S9Bv+WLl26T0S2NmdZA0gH2uoN5m1132y/Ik9b3bfWuF/9Ak2IuLaGWoqI5ARqlyPStdV9s/2KPG113yJtv+zJYmOMiXIWCIwxJspZIAjs2XAXIITa6r7ZfkWetrpvEbVfVkdgjDFRzq4IjDEmylkgMMaYKGeBABCR50Vkr4is9hrXWUT+LSLr3b+dwlnGxhCRPiIyT0TWisgaEbndHR/R+yYiSSLyHxFZ6e7Xr93xmSLytdvs+Rvug4wRSURiRWS5iLzvDkf8vonIFhH5RkRWiEiOOy6iv4sAIpImIm+JyHci8q2InBZp+2WBwDELGOcz7l7gU1UdCHzqDkeacuAuVR2K04THzSIylMjft6PAOao6AsgCxonIqTit2P5JVY8FDgDXh7GMTXU78K3XcFvZt7NVNcvrHvtI/y4CPA58pKrHASNw/m+RtV+qap1TYd4fWO01vA7o6fb3BNaFu4zNsI/v4rwfos3sG5AMLANOwXmSM84dfxrwcbjL18h9ysA5eJwDvA9IW9g3YAuQ7jMuor+LQEdgM+6NN5G6X3ZFEFh3Vd3l9u8GuoezME0lIv2BkTgvAIr4fXNTJyuAvcC/gY1AgaqWu7PUavY8gjwG3ANUusNdaBv7psAnIrJURKa64yL9u5gJ5AEvuKm850QkhQjbLwsEQVAnrEfsfbYi0h54G5imqkXe0yJ131S1QlWzcM6eTwaOC3ORmoWI/BDYq6pLw12WEDhDVUcBF+KkKcd4T4zQ72IcMAr4i6qOBA7hkwaKhP2yQBDYHhHpCeD+3Rvm8jSKiMTjBIFXVfUf7ug2sW8AqloAzMNJl6SJiKchRd9mzyPF6cClbtPss3HSQ4/TBvZNVXe4f/cC/8QJ4JH+XcwFclXV86rdt3ACQ0TtlwWCwOYAk93+yTj59YgiIgL8DfhWVf/oNSmi901EuopImtvfDqfe41ucgHClO1vE7ReAqt6nqhnqNM0+AfhMVScR4fsmIinu+8lxUyfnA6uJ8O+iqu4GtovIYHfUucBaImy/7MliQEReB8biNB27B3gIeAd4E+gLbAWuUtX94SpjY4jIGcAi4Buq883349QTROy+ichw4EWc5s1jgDdV9WEROQbnLLozsBy4RlWPhq+kTSMiY4G7VfWHkb5vbvn/6Q7GAa+p6gwR6UIEfxcBRCQLeA5IADYB1+J+L4mQ/bJAYIwxUc5SQ8YYE+UsEBhjTJSzQGCMMVHOAoExxkQ5CwTGGBPlLBCYNkNEVEQe9Rq+W0Smh2A7r4vIKhG5w2d8V7eF0OUicmYD15klIhc1b0mNCY4FAtOWHAWuEJH0UG1ARHoAJ6nqcFX9k8/kc4FvVHWkqi5q4KqzgAYFAq8njY1pEgsEpi0px3lX7B2+E0Skv4h85p7JfyoifetakfvOgxfc9vOXi8jZ7qRPgN5um/pnes2fBfwfcJk7rZ2I/EVEcrzfmeDOe5KILHbfp/AfEekIPAyMd5cd77Zn/45b3q/ch+gQkeki8rKIfAG8LCLHu+tY4c47sImfoYlG4W7+1DrrmqsDDgIdcJo77gjcDUx3p70HTHb7rwPeqWdddwHPu/3HAduAJHyaK/dZZgrwlNdwZ/dvLDAfGE7106cnudM64Dxp67vsk8BDbv85wAq3fzqwFGjnNd8ktz/BM9466xrS2RWBaVPUaV31JeA2n0mnAa+5/S8DZ9SzqjOAV9x1fofTTMCgBhbnKhFZhtMkxPHAUGAwsEtVl3jKq9XNS/tu/2V3ns+ALiLSwZ02R1WPuP1fAveLyP8A/bzGGxM0CwSmLXoM5w1eKeEqgIhk4lyRnKuqw4F/4VxRNIdDnh5VfQ24FDgCfCAi5zTTNkwUsUBg2hx1Gvd6k5qvc1yM05onwCScxvjqssidDxEZhNN42LoGFKMDzgG7UES647TBj7uOniJykrvuVLfStxhIDbD9scA+9XmXhDvtGGCTqj6B08Ll8AaU0RjAAoFpux7FaU3W41bgWhFZBfwU553AiMiNInKjn+X/DMSIyDfAG8AUbUBrn6q6Eicl9B1OSuoLd3wpMB54UkRW4rxdLQmnmemhnspinLqAE93yPkJ1k8a+rgJWu29rG4aTFjOmQaz1UWOMiXJ2RWCMMVHOAoExxkQ5CwTGGBPlLBAYY0yUs0BgjDFRzgKBMcZEOQsExhgT5f4/kJDIh9d+vkEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYTtH0_pl6sj"
      },
      "source": [
        "## Evaluating Other Models\n",
        "\n",
        "When evaluating models, it's important to compare to some reasonable baselines. \n",
        "\n",
        "Fortunately, Spotlight's `rmse_score()` method can be used to evaluate any Python object that adheres to the specification of the `predict()` function. For instance, we can make a baseline \"static\" scoring model, which returns the same scores for each user. This set of scores is passed as numpy array in the constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Xd1X4TX2Tq"
      },
      "source": [
        "class StaticModel:\n",
        "  \n",
        "  def __init__(self, staticscores):\n",
        "    self.numitems = len(staticscores)\n",
        "    self.staticscores = staticscores\n",
        "  \n",
        "  #uids are the user(s) we are requesting recommendations for;\n",
        "  #returns an array of scores, one for each item\n",
        "  #the array is duplicated for each user requested\n",
        "  def predict(self, uids, iids=None):\n",
        "    #this model returns all zeros, regardless of userid\n",
        "    \n",
        "    #we respond to one or more uids\n",
        "    uids = [uids] if isinstance(uids, int) else uids\n",
        "\n",
        "    #if iids is specificed, we filter predicts for those userids\n",
        "    iids = np.arange(self.numitems) if iids is None else iids\n",
        "    return [self.staticscores[iids] for u in uids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhl_7mUxmdlv"
      },
      "source": [
        "For instance, we can make a static baseline that just returns 0 for every item, regardless of the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3FVJWJzYhoC",
        "outputId": "13e93fc6-175d-474d-e708-482d1a589049"
      },
      "source": [
        "mydummymodel = StaticModel(np.zeros(num_items))\n",
        "\n",
        "print(\"Asking for 2 users, one item: \" + str(mydummymodel.predict([0,1],0)))\n",
        "print(\"Asking for one item: \" + str(mydummymodel.predict(0,0)))\n",
        "print(\"Asking for two items: \" + str(mydummymodel.predict(0,[0,1])))\n",
        "print(\"RMSE of our dummy model: %f\" % rmse_score(mydummymodel, test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Asking for 2 users, one item: [0.0, 0.0]\n",
            "Asking for one item: [0.0]\n",
            "Asking for two items: [array([0., 0.])]\n",
            "RMSE of our dummy model: 3.642758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSnTtOzAoA2l"
      },
      "source": [
        "## Task 6. Popularity-based Recommenders\n",
        "\n",
        "This task asks you to implement other baseline recommenders.\n",
        "\n",
        "**Using ratings_df**, create three new instances of StaticModel as baselines:\n",
        "\n",
        "(a). the number of ratings for each item - you must linearly normalise this to be in the range 0-5.\n",
        "\n",
        "(b). the number of 5 scores received by an item - you must linearly normalise this to be in the range 0-5.\n",
        "\n",
        "(c). the average rating value for each item (no need to normalise - scores are already 0-5)\n",
        "\n",
        "Evaluate your baseline models in terms of RMSE, as well as providing their scores for particular iids, as requested in the quiz.\n",
        "\n",
        "Hints:\n",
        " - You may find iterating over a dataframe using iterrows() useful - e.g. see  https://stackoverflow.com/a/16476974\n",
        " - Order is VERY IMPORTANT. Think carefully about the assumed order that predict() returns item scores for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7ff_L2dMAGO",
        "outputId": "506d792c-11e7-4b5b-d8d8-59c232c62189"
      },
      "source": [
        "#6A\n",
        "ratings=np.zeros(num_items)\n",
        "mydf=ratings_df.groupby(by=['movieId']).count()\n",
        "for i,j in mydf.iterrows():\n",
        "  ratings[iid_map[i]]=j['rating']\n",
        "ratings=(ratings/max(ratings)*5)\n",
        "model1=StaticModel(ratings)\n",
        "print(\"RMSE of my 1st dummy model 1 is : %f\" % rmse_score(model1,test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE of my 1st dummy model 1 is : 2.887788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaledZkCQxTB",
        "outputId": "54c28243-ba8a-4b55-dc6c-fa8797903d3f"
      },
      "source": [
        "#6A\n",
        "print(\"Asking for one item: \" + str(model1.predict(0,0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Asking for one item: [3.2674772036474167]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swTaUCjBfJBZ",
        "outputId": "65972b96-5102-48d9-bcb7-0f78278016ad"
      },
      "source": [
        "#6B\n",
        "ratings_df[(ratings_df['movieId']=='m10') & (ratings_df['rating']==5)].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "userId       10\n",
              "movieId      10\n",
              "rating       10\n",
              "timestamp    10\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAKm4-fTaWEa",
        "outputId": "00b454e0-5f4e-4956-f85f-5111338689a6"
      },
      "source": [
        "#6B\n",
        "ratings=np.zeros(num_items)\n",
        "mydf=ratings_df[ratings_df['rating']==5]\n",
        "mydf=mydf.groupby(['movieId']).count()\n",
        "for i,j in mydf.iterrows():\n",
        "  ratings[iid_map[i]]=j['rating']\n",
        "ratings=(ratings/max(ratings)*5)\n",
        "model2=StaticModel(ratings)\n",
        "print(\"RMSE of my 1st dummy model 1 is : %f\" % rmse_score(model2,test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE of my 1st dummy model 1 is : 3.308017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "555zyBXadU0c",
        "outputId": "72dd1bae-5703-429d-f1c6-2458d6717173"
      },
      "source": [
        "#6B\n",
        "print(\"Asking for one item: \" + str(model2.predict(0,0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Asking for one item: [1.5359477124183007]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWneY14rTPE9",
        "outputId": "7edb2c0f-035c-40b1-a1e5-5d2451a6a192"
      },
      "source": [
        "#6C\n",
        "ratings=np.zeros(num_items)\n",
        "mydf=ratings_df.groupby(by=['movieId']).mean()\n",
        "for i,j in mydf.iterrows():\n",
        "  ratings[iid_map[i]]=j['rating']\n",
        "#ratings=(ratings/max(ratings)*5)\n",
        "model3=StaticModel(ratings)\n",
        "print(\"RMSE of my 1st dummy model 3 is : %f\" % rmse_score(model3,test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE of my 1st dummy model 3 is : 0.881075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z3CXaWZU7m0",
        "outputId": "1e4372e4-3fb3-44db-9cc2-5d06e711624c"
      },
      "source": [
        "#6C\n",
        "print(\"Asking for one item: \" + str(model3.predict(0,0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Asking for one item: [3.9209302325581397]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCiYTWaeobQ"
      },
      "source": [
        "# Part C - Implicit Recommendation\n",
        "\n",
        "This part of the lab uses a music dataset from [Last.fm](https://www.last.fm/) -- a Spotify-like music streaming service -- that was obtained by a researcher at Pompeu Fabra University (Barcelona, Spain). The relevant citation is:\n",
        "\n",
        "```\n",
        "  @book{Celma:Springer2010,\n",
        "      \tauthor = {Celma, O.},\n",
        "      \ttitle = {{Music Recommendation and Discovery in the Long Tail}},\n",
        "       \tpublisher = {Springer},\n",
        "       \tyear = {2010}\n",
        "      }\n",
        " ```\n",
        "\n",
        "You can have more information about the dataset at [this link](http://ocelma.net/MusicRecommendationDataset/lastfm-1K.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsDavoa3qC64"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "This dataset is 600MB copmressed, and 2.4GB uncompressed. It takes 30 seconds to download on Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-drWmULel_p",
        "outputId": "a480e05a-d4be-4305-cb5f-546eece0a5fd"
      },
      "source": [
        "!rm -rf lastfm-dataset-1K.tar.gz\n",
        "!curl -o \"lastfm-dataset-1K.tar.gz\" \"http://www.dcs.gla.ac.uk/~craigm/recsysH/lastfm-dataset-1K.tar.gz\"\n",
        "#backup location\n",
        "#!curl -o \"lastfm-dataset-1K.tar.gz\" http://macavaney.us/misc/lastfm-dataset-1K.tar.gz\n",
        "!tar -zxvf lastfm-dataset-1K.tar.gz\n",
        "!ls -lh lastfm-dataset-1K/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  641M  100  641M    0     0  27.4M      0  0:00:23  0:00:23 --:--:-- 28.9M\n",
            "lastfm-dataset-1K/\n",
            "lastfm-dataset-1K/userid-profile.tsv\n",
            "lastfm-dataset-1K/README.txt\n",
            "lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\n",
            "total 2.4G\n",
            "-rw-r--r-- 1 1002 1002 2.2K Mar 23  2010 README.txt\n",
            "-rw-r--r-- 1 1002 1002  37K Dec 30  2009 userid-profile.tsv\n",
            "-rw-r--r-- 1 1002 1002 2.4G Mar  4  2010 userid-timestamp-artid-artname-traid-traname.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxILh_-h773"
      },
      "source": [
        "listens_df = pd.read_csv(\"lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\",  names=['user', 'timestamp', 'artistid', 'artist', 'trackid', 'trackname'], header=None, sep='\\t')\n",
        "\n",
        "#Some tracks dont seem to have artists or track names, so lets drop them for simplicity.\n",
        "listens_df = listens_df[listens_df.artist.notnull()]\n",
        "listens_df = listens_df[listens_df.trackname.notnull()]\n",
        "\n",
        "#the dataframe is VERY big (19M interactions), so lets just work with a small sample of it (this will mean that effectiveness will be lower, but learning will be MUCH faster).\n",
        "listens_df = listens_df.sample(n=200000, random_state=np.random.RandomState(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZSwFnZSgrNU"
      },
      "source": [
        "\n",
        "Let's look at the dataset. Note that the we don't have any explicit ratings by the users. We just know what they interacted with (and when). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "HAP3dPt-4KMi",
        "outputId": "9b59f49b-fd42-41df-e984-51ecfeca4f37"
      },
      "source": [
        "listens_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>artistid</th>\n",
              "      <th>artist</th>\n",
              "      <th>trackid</th>\n",
              "      <th>trackname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11087179</th>\n",
              "      <td>user_000593</td>\n",
              "      <td>2007-05-14T18:49:03Z</td>\n",
              "      <td>ad996aef-cc1c-42ac-af5c-619c370f4b8a</td>\n",
              "      <td>Emerson, Lake &amp; Palmer</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Three Fates (Clotho/Lachesis/Atropos)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1911790</th>\n",
              "      <td>user_000093</td>\n",
              "      <td>2008-08-18T22:04:59Z</td>\n",
              "      <td>8c538f11-c141-4588-8ecb-931083524186</td>\n",
              "      <td>Bloc Party</td>\n",
              "      <td>315a301e-e764-4adf-91c6-e90a22320106</td>\n",
              "      <td>Positive Tension</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11099786</th>\n",
              "      <td>user_000594</td>\n",
              "      <td>2008-04-06T10:57:45Z</td>\n",
              "      <td>65f4f0c5-ef9e-490c-aee3-909e7ae6b2ab</td>\n",
              "      <td>Metallica</td>\n",
              "      <td>683c89fe-2be8-4ed2-8e58-68b2343cb8d5</td>\n",
              "      <td>Through The Never</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12076983</th>\n",
              "      <td>user_000651</td>\n",
              "      <td>2008-05-10T07:14:45Z</td>\n",
              "      <td>3ca09fae-fdee-4771-bab9-244708515a98</td>\n",
              "      <td>Omarion</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ice Box [Orangefuzzz Weather Advisory Radio Mix]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2680461</th>\n",
              "      <td>user_000137</td>\n",
              "      <td>2009-03-11T23:17:22Z</td>\n",
              "      <td>af84ee9f-534a-4f7f-844b-188ba1c47e87</td>\n",
              "      <td>Los Rodríguez</td>\n",
              "      <td>76b83f07-3763-4c17-8d24-28040d85354a</td>\n",
              "      <td>Dulce Condena</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 user  ...                                         trackname\n",
              "11087179  user_000593  ...             Three Fates (Clotho/Lachesis/Atropos)\n",
              "1911790   user_000093  ...                                  Positive Tension\n",
              "11099786  user_000594  ...                                 Through The Never\n",
              "12076983  user_000651  ...  Ice Box [Orangefuzzz Weather Advisory Radio Mix]\n",
              "2680461   user_000137  ...                                     Dulce Condena\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGzG9Rig3E6"
      },
      "source": [
        "## An implicit recommendation approach\n",
        "\n",
        "Let's move away from explicit recommendation to implicit.\n",
        "\n",
        "We will continue using the [Spotlight](https://github.com/maciejkula/spotlight/) toolkit for our recommender. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U03zZ6CH---1"
      },
      "source": [
        "We can construct [Interaction](https://maciejkula.github.io/spotlight/interactions.html) objects for Spotlight in the same way as before. The only difference is that this time we do not record the user's ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcRhNWXzg7LT",
        "outputId": "35c70d3f-ca22-48e5-d26f-e38da584609e"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "#we cant trust the musicbrainz ids to exist, so lets build items ids based on artist & trackname attributes\n",
        "LFMiid_map = defaultdict(count().__next__)\n",
        "LFMiids = np.array([LFMiid_map[artist+\"/\"+trackname] for artist,trackname in listens_df[[\"artist\",\"trackname\"]].values ], dtype=np.int32)\n",
        "\n",
        "LFMuid_map = defaultdict(count().__next__)\n",
        "LFMuids = np.array([LFMuid_map[uid] for uid in listens_df[\"user\"].values ], dtype=np.int32)\n",
        "#freeze uid_map and iid_map so no more mapping are created\n",
        "LFMuid_map.default_factory = None\n",
        "LFMiid_map.default_factory = None\n",
        "\n",
        "LFMuid_rev_map = {v: k for k, v in LFMuid_map.items()}\n",
        "LFMiid_rev_map = {v: k for k, v in LFMiid_map.items()}\n",
        "\n",
        "from spotlight.interactions import Interactions\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "#NB: we will set num_users and num_items here - its a good practice.\n",
        "imp_dataset = Interactions(user_ids=LFMuids, item_ids=LFMiids, num_users=len(LFMuid_map), num_items=len(LFMiid_map))\n",
        "#we could add the timestamps here if we were doing sequence recommendation\n",
        "\n",
        "#what have we got.\n",
        "print(imp_dataset)\n",
        "LFMiids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Interactions dataset (973 users x 125076 items x 200000 interactions)>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     0,      1,      2, ..., 125074, 125075,   2602], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GwYKxnv24VKq",
        "outputId": "c18b5c4c-4b48-4c52-ab67-fa1294efcb07"
      },
      "source": [
        "LFMiid_rev_map.get(LFMiids[2602])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Mariah Carey/We Belong Together (Peter Rauhofer Radio Edit)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dmr7JKqUnz",
        "outputId": "e653e099-7a00-4ed0-9e10-80c24f13e285"
      },
      "source": [
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "itrain, itest = random_train_test_split(imp_dataset, random_state=np.random.RandomState(SEED))\n",
        "print(itrain)\n",
        "print(itest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Interactions dataset (973 users x 125076 items x 160000 interactions)>\n",
            "<Interactions dataset (973 users x 125076 items x 40000 interactions)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFQazPPxhG2_"
      },
      "source": [
        "Let's run Spotlight's impllicit Matrix Factorisation on this dataset. Here, we use a *pointwise* loss, which just tries to predict whether the user will like the item or not. It does not use the BPR loss function (more on that later).\n",
        "\n",
        "**Warning**: this dataset is difficult for the learner - this *will* take a few minutes to learn... Use the time to read-on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co3ZwYgkhKvq",
        "outputId": "644ca837-693c-4380-9437-230b69f928bd"
      },
      "source": [
        "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
        "import time  \n",
        "\n",
        "imodel = ImplicitFactorizationModel(n_iter=5, \n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False,\n",
        "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
        ")\n",
        "current = time.time()\n",
        "\n",
        "imodel.fit(itrain, verbose=True)\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds\" % (diff))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 0.9663112537384033\n",
            "Epoch 1: loss 0.4953250964164734\n",
            "Epoch 2: loss 0.19036926743984223\n",
            "Epoch 3: loss 0.11518938970565797\n",
            "Epoch 4: loss 0.08347186335921288\n",
            "Training took 164 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zso4C5wehLog"
      },
      "source": [
        "Again, we can look at the predictions. We make a prediction (a score ) for ALL items for user uid 0. Note that the scores vary in magnitude - indeed, we're not predicting a rating, we just need to have scores in order to rank the items in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR_qbWXEhUDB",
        "outputId": "24f1fa02-f5d0-4aa7-eda2-bb304ee87fa5"
      },
      "source": [
        "print(imodel.predict(4))\n",
        "print(len(imodel.predict(4)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ -8.309335    2.8038096  -7.2378526 ...  -4.77122    -9.353735\n",
            " -10.95487  ]\n",
            "125076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyZ3PyAxhdDF"
      },
      "source": [
        "Now that we have the scores of all items for a given user, we need to identify the top-scored ones, i.e. those that we would present to the user. \n",
        "\n",
        "## Task 7. Track Analysis\n",
        "\n",
        "Write a function `tracksForUser(user)` to identify the artist name & track of the top K (e.g. K=4) items based on their score for a given user index index (i.e. 0.. 964). What are the top scored 10 tracks recommended for user uid 4?\n",
        "\n",
        "Hints: \n",
        "\n",
        " \n",
        " - I also found [`np.argwhere()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argwhere.html) to be useful. It results only the positions of an array that are True. For instance:\n",
        "```\n",
        ">>> np.argwhere([True, False])\n",
        "array([[0]])\n",
        "```\n",
        " Alternatively, you can sort and then slice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V93_Bxw3MG1F",
        "outputId": "81a89fd2-9eb2-4a8b-cdfb-5bb9b47fa797"
      },
      "source": [
        "def tracksForUser(user):\n",
        "  k=10\n",
        "  y=imodel.predict(user)\n",
        "  rank=rankdata(y, method='ordinal').astype(int)\n",
        "  rank=len(rank) - rank\n",
        "  rank=rank+1\n",
        "  rank=rank.tolist()\n",
        "  for s in range(1,k+1):\n",
        "    r=rank.index(s)\n",
        "    print(LFMiid_rev_map.get(r))\n",
        "tracksForUser(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evanescence/Sweet Sacrifice\n",
            "Mgmt/Kids\n",
            "The Killers/Bones\n",
            "Nelly Furtado/Say It Right\n",
            "Kings Of Leon/Use Somebody\n",
            "Amy Winehouse/Back To Black\n",
            "Red Hot Chili Peppers/The Zephyr Song\n",
            "Radiohead/Fake Plastic Trees\n",
            "Incubus/Drive\n",
            "Him/The Funeral Of Hearts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cof_VEFD9kvY",
        "outputId": "0caf48de-d919-45e0-a57b-f08ed21db246"
      },
      "source": [
        "LFMiid_rev_map.get(LFMiids[7010])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Blood On The Wall/The X'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWFSAuQ40p2Y"
      },
      "source": [
        "## Task 8. Artist Analysis\n",
        "\n",
        "Look at the artists actually listened to by uid 4, and compare/contrast with the predictions of the recommender. It's useful to examine how many times each artist was listened to.\n",
        "\n",
        "Hints: \n",
        " - use a groupby on a suitable subset of the listens_df dataframe. \n",
        " - Sort by descending frequency of listen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "T2wFfGfcMJcc",
        "outputId": "5b863640-b8d9-499a-fcd3-0a0c2462cb7d"
      },
      "source": [
        "x=listens_df[listens_df['user']==LFMuid_rev_map.get(4)]\n",
        "x=x[['user','artist','trackname']].groupby(by=['artist']).count()\n",
        "x.sort_values(by=['user'], ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>trackname</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>artist</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Soda Stereo</th>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gustavo Cerati</th>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Radiohead</th>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lucybell</th>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Silvio Rodríguez</th>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coldplay</th>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dream Theater</th>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Muse</th>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Daniel Garcia</th>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pearl Jam</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  user  trackname\n",
              "artist                           \n",
              "Soda Stereo         39         39\n",
              "Gustavo Cerati      36         36\n",
              "Radiohead           31         31\n",
              "Lucybell            27         27\n",
              "Silvio Rodríguez    16         16\n",
              "Coldplay            13         13\n",
              "Dream Theater       13         13\n",
              "Muse                11         11\n",
              "Daniel Garcia       11         11\n",
              "Pearl Jam           10         10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FWymqQRxKj"
      },
      "source": [
        "I observed that uid 4 listened frequently to \"Radiohead\" (rank 3), while a Radiohead song was among the top 10 ranked songs in our predicted model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTNae6Romuk"
      },
      "source": [
        "## Evaluating an implicit recommender\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Q2TTpZuHON"
      },
      "source": [
        "We can examine the MRR of the implicit model we have learned. We pass it the test set (which contains knowledge of what the user *actually* clicked), as our ground truth. \n",
        "\n",
        "In the second variant, we also pass the training data. Give a look at the  implementation of [mrr_score()](https://github.com/cmacdonald/spotlight/blob/master/spotlight/evaluation.py#L8) to understand what it is doing, and why.\n",
        "\n",
        "**Questions for you to consider**\n",
        " - Why is the second score lower? \n",
        " - Would this be the same for all recommendation settings? \n",
        " - In the implementation, why are the scores negated, why do we use [rankdata()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html)?\n",
        " \n",
        "We will use the first variant for this Lab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLV71LSjt-k2",
        "outputId": "ec7cb66c-7a1d-49d3-e42d-bcb7960ef0c6"
      },
      "source": [
        "from spotlight.evaluation import mrr_score\n",
        "\n",
        "#evaluate on this dataset takes approx 1 minute\n",
        "!date\n",
        "print(mrr_score(imodel, itest).mean())\n",
        "!date\n",
        "print(mrr_score(imodel, itest,  train=itrain).mean())\n",
        "!date\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug  4 14:19:13 UTC 2021\n",
            "0.03720125940064275\n",
            "Wed Aug  4 14:19:50 UTC 2021\n",
            "0.008104536778740273\n",
            "Wed Aug  4 14:20:27 UTC 2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjM_kZNtAE2k"
      },
      "source": [
        "How to interpret an MRR score - we know it has a range [0,1] with 1 being best. 1 means, on average across all users, we make a relevant prediction at rank 1; 0.5 means, on average, at rank 2. This is a very rough rule-of-thumb - MRR isn't a linear measure, so  a few poor predictions affect the average more than a few good ones.\n",
        "\n",
        "\n",
        "You can now answer all questions for Task 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL2jRiGF2uLb"
      },
      "source": [
        "## Task 9. Listens and Recommendations\n",
        "\n",
        "*   Pick the user with the lowest uid that has RR=1 (you should not specify `train=` when making this choice). How many listens (ie. how many times they have listened to any song) did they have in the training dataset?\n",
        "*   Similarly, pick the user with the lowest uid that had the lowest RR. How many listens did they have in the training dataset?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0gxFQJVtR6e"
      },
      "source": [
        "all_mrr = mrr_score(imodel, itest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6H0l1S6nNcd",
        "outputId": "3491cf77-6414-4246-e673-98b31fd62e35"
      },
      "source": [
        "i = np.where(all_mrr == 1)\n",
        "print(all_mrr)\n",
        "print(i)\n",
        "print(\"1st user with rr=1:\",LFMuid_rev_map.get(i[0][0]))\n",
        "#len(itrain.item_ids[itrain.user_ids == i[0][0]])\n",
        "min_i=min(i)[0]\n",
        "print(\"min_id with 1 mrr is \",min_i)\n",
        "print(len(itrain.item_ids[itrain.user_ids == min_i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2        0.         0.33333333 0.         0.25       0.\n",
            " 0.         0.5        0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.11111111 0.\n",
            " 0.14285714 0.         0.         0.         0.         0.25\n",
            " 0.         0.         0.         0.16666667 0.         0.\n",
            " 0.         1.         0.         0.33333333 0.         0.14285714\n",
            " 1.         0.         0.         0.         0.         0.\n",
            " 0.5        0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         1.\n",
            " 0.         0.         0.         0.         0.         0.1\n",
            " 0.1        0.         0.         0.16666667 1.         0.\n",
            " 0.         1.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         1.         0.         0.\n",
            " 0.         0.         0.11111111 0.         0.         0.\n",
            " 0.         0.5        0.         0.         0.         0.\n",
            " 0.         0.14285714 0.         0.         0.14285714 0.5\n",
            " 0.         0.5        0.         0.         0.         0.\n",
            " 0.         0.125      0.         0.         0.         0.\n",
            " 0.         0.25       0.         0.         0.         0.\n",
            " 0.         0.1        0.         0.         0.5        0.\n",
            " 0.         0.         0.         0.         0.11111111 0.\n",
            " 0.         0.         0.         0.5        0.         1.\n",
            " 0.5        0.         0.         0.         0.         0.\n",
            " 0.5        0.         0.         0.         0.         0.1\n",
            " 0.         0.         0.2        0.         0.         0.\n",
            " 0.         1.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.33333333 0.         0.16666667 0.         0.\n",
            " 0.33333333 0.         0.         0.         0.         0.5\n",
            " 0.         0.16666667 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         1.         0.         0.\n",
            " 0.         0.         0.5        0.5        1.         0.33333333\n",
            " 0.         0.         0.         0.14285714 0.         0.\n",
            " 0.16666667 0.         0.         0.125      0.         0.14285714\n",
            " 0.16666667 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.1        0.         0.         0.         0.         0.\n",
            " 0.2        0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.33333333 0.         0.         0.         0.         0.\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.1\n",
            " 0.11111111 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.33333333 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.16666667 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.11111111 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.2        0.         0.25       0.\n",
            " 0.125      0.         0.         0.         0.         0.\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.11111111 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         1.         0.         0.         0.         0.\n",
            " 0.25       0.         0.         0.         0.         0.\n",
            " 0.125      0.         0.         0.         0.         0.1\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.25       0.         0.         0.         0.\n",
            " 0.         0.         0.16666667 0.         0.         0.\n",
            " 0.125      0.         0.         0.         0.         0.\n",
            " 0.         0.125      0.         0.         0.16666667 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.125      0.         0.         0.         0.\n",
            " 0.         0.5        0.         0.         0.         0.\n",
            " 0.33333333 0.1        0.         0.         0.11111111 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.125      0.         0.         0.\n",
            " 0.         1.         0.         0.11111111 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.14285714 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.25       0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         1.\n",
            " 0.         0.         0.25       0.         0.         0.\n",
            " 0.         0.         0.14285714 0.11111111 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         1.         0.         0.         0.         0.\n",
            " 0.         0.2        0.         0.         0.         0.\n",
            " 0.         0.         0.33333333 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.125      0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.125      0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11111111 0.         0.         0.\n",
            " 0.         0.         0.         0.125      0.         0.\n",
            " 0.         0.         0.         0.         0.         0.125\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.25       0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.33333333 0.         0.         0.         0.14285714\n",
            " 0.         0.125      0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.16666667 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.5        0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.125\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]\n",
            "(array([ 31,  36,  53,  64,  67,  81, 137, 157, 201, 208, 260, 356, 385,\n",
            "       499, 569, 613]),)\n",
            "1st user with rr=1: user_000833\n",
            "min_id with 1 mrr is  31\n",
            "757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bttvugz7tTxX",
        "outputId": "ef308271-1316-497c-c748-a73d70579d09"
      },
      "source": [
        "j = np.where(all_mrr == 0)\n",
        "#print(j)\n",
        "print(\"1st user with rr=1:\",LFMuid_rev_map.get(j[0][0]))\n",
        "min_j=min(j)[0]\n",
        "print(\"min_id is with 0 mrr \",min_j)\n",
        "print(len(itrain.item_ids[itrain.user_ids == min_j]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st user with rr=1: user_000093\n",
            "min_id is with 0 mrr  1\n",
            "355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anEhZSz9nPZt"
      },
      "source": [
        "Next, make a numpy array containing the number of listens for each uid in the LastFM dataset. Plot a histogram of the distribution - like in Exercise 1, use matplotlib's histogram functionality, the default number of bins and use `log=True`. \n",
        "\n",
        "Save the PNG for uploading to the quiz when prompted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "vvPY4seNpBZE",
        "outputId": "fa319836-e48a-42c8-9816-53bc4046ae47"
      },
      "source": [
        "total = []\n",
        "for i in range(len(all_mrr)):\n",
        "  total.append(len(itrain.item_ids[itrain.user_ids == i]))\n",
        "total = np.array(total)\n",
        "#plt.figure(figsize=(7,7))\n",
        "plt.hist(total, log=True)\n",
        "plt.xlabel('Number of tracks')\n",
        "plt.ylabel('Number of listens')\n",
        "plt.savefig('ListenDF.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCI34-U48HHI",
        "outputId": "9ed63289-a16b-4720-85b3-7f03c2651893"
      },
      "source": [
        "new_users = []\n",
        "for i in range(len(all_listens)):\n",
        "  if all_listens[i] < 20:\n",
        "    new_users.append(i)\n",
        "mrr_new_users, mrr_normal = {}, {}\n",
        "for i in range(len(all_mrr)):\n",
        "  if i in new_users:\n",
        "    mrr_new_users[i] = all_mrr[i]\n",
        "  else:\n",
        "    mrr_normal[i] = all_mrr[i]\n",
        "total_new=0\n",
        "for i,j in mrr_new_users.items():\n",
        "  total_new += j\n",
        "print(total_new/len(mrr_new_users))\n",
        "total_normal=0\n",
        "for i,j in mrr_normal.items():\n",
        "  total_normal += j\n",
        "print(total_normal/len(mrr_normal))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0024250440917107582\n",
            "0.044147926330417094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kjpISm3nwAy"
      },
      "source": [
        "Many users have very few listens. Lets set 20 listens as a threshold.\n",
        "\n",
        "Lets define users with < 20 listens as cold-start users.\n",
        "How many cold-start users are there?\n",
        "What is the MRR for ONLY these users, versus \"normal\" with 20 or more listens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR_l43rFc7eo"
      },
      "source": [
        "## Task 10 - BPR\n",
        "\n",
        "Finally, let's compare the *pointwise* implicit factorisation model with *BPR*. BPR is a very key recommendation model in the literature, which is widely used today as a baseline in many research papers.\n",
        "\n",
        "Train an ImplicitFactorizationModel on the Last FM dataset (i.e. `itrain`) using identical settings as before, except adding `loss='bpr'`. Record the time taken to train, and the evaluate its effectiveness in terms of MRR. Do NOT use the `train=itrain` argument to `mrr_score()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oiCB4PuMQ_f",
        "outputId": "2bd838fc-1bb9-4871-e7d2-c1238d95f4d5"
      },
      "source": [
        "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
        "import time  \n",
        "\n",
        "imodel_2 = ImplicitFactorizationModel(n_iter=5, \n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False,\n",
        "                                    loss='bpr',\n",
        "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
        ")\n",
        "current = time.time()\n",
        "\n",
        "imodel_2.fit(itrain, verbose=True)\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds\" % (diff))\n",
        "print(round(mrr_score(imodel_2, itest).mean(),4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss 0.4740972354888916\n",
            "Epoch 1: loss 0.14683696522712708\n",
            "Epoch 2: loss 0.024808155296742917\n",
            "Epoch 3: loss 0.014375447143614292\n",
            "Epoch 4: loss 0.01111387666836381\n",
            "Training took 165 seconds\n",
            "0.0581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTPM_BGcUc0"
      },
      "source": [
        "# End of Exercise\n",
        "\n",
        "As part of your submission, you should complete the Exercise 2 quiz on Moodle.\n",
        "You will need to upload your notebook, complete with the **results** of executing the code (including figures and plots)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nxzkq2EbAmJl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}